\chapter{Characteristics of Neural Networks}\label{char_nn}
\section{Introduction}
In this chapter, we will explore the characteristics of NN models we want to measure and analyse. From now on, we will use the term \textit{characteristics} to identify all the metrics of the model which will have an influence on its performances once the model is deployed. The term is therefore an umbrella term for any type of measurable property which we can use for our application. In the following sections we will then take a closer look to each one of them with the purpose of understanding them. 

\section{Learning Process and Training Time}\label{sec:training_time}
Training time, as the name implies, is the time required to train a Neural Network, or any machine learning model. \textit{Mitchell} in \cite{machine_learning} provides a general definition for the training, or learning, process : \textit{“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”} This definition is quite broad (\cite{Goodfellow-et-al-2016}) and it is out of the scope of this paper to give a formal definition for the experience E, task T, and performance measure P.\\More practically, the process of training is the process of updating the weights of the neural network to so that the network is able to achieve the desired goal. \cite{murphy2016overview} The network weights are updated during back propagation by the following equation(\cite{murphy2016overview}):
\begin{equation}
W_{new} = W - \eta \frac{dE}{dW}
\end{equation}
The error, or ''scoring'', function is defined as the sum of squared differences between the network's output(equation n. \ref{eq:MSE}) and the correct output and it is applicable when the output is vector, matrix, or tensor of continuous real values (\cite{murphy2016overview}). 
\begin{equation}
E(W,b) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{2} \vert \vert h_{W,b} (x^{(i)}) - y^{(i)} \vert \vert ^{2}
\label{eq:MSE}
\end{equation}
The learning experience can be broadly categorized into two main categories: supervised and unsupervised learning. Roughly speaking, given a dataset $\mathcal{D}$ , an unsupervised learning algorithm aims to learn useful properties of this dataset. In the context of deep learning, we aim to learn the entire probability distribution $p(\mathcal{D})$ that generated the dataset or some useful properties of it. On the other hand, supervised learning implies the use of a dataset $\mathcal{D}$ and an associated value or label $\mathbb{Y}$ to teach the model to predict $\mathbb{Y}$ from $\mathbb{X} \subset \mathcal{D}$, usually by estimating $p(\mathbb{Y} \vert \mathbb{X})$(\cite{murphy2016overview})\\
Usually the learning process proceeds in waves of mini-batches, which allow to avoid both over-fitting and under utilization of GPU’s compute parallelism, therefore throughput is a key performance metrics to measure.\cite{8573476}
Finally, the learning process is a memory demanding one, as  backward pass and weight updates, operations unique to training, need to save intermediate results in GPU memory (in some cases tens of gigabytes are required). \cite{rhu2016vdnn}
Training time is often an overlooked metrics of Neural Network compared to inference(\ref{sec:inference_time_definition}) or accuracy (\ref{sec:accuracy}) (\cite{8573476}), however, due to the recent growth in applications of Deep Learning technologies in different fields (\cite{bojarski2016end}, \cite{huval2015empirical}, \cite{10.1145/2959100}, \cite{amodei2015deep}) training time is acquiring importance. \cite{8573476}\\
\section{Inference Time}\label{sec:inference_time_definition}
In Machine Learning, therefore in Deep Learning as well, the term \textbf{inference time} is used to indicate the time required for the trained model to make predictions, regardless of the correctness of those. \\
Inference time has attracted a lot of attention in the research field, since execute already trained Neural Networks efficiently is inarguably a still open problem. \cite{8573476}\\
Differently from Training Time (section \ref{sec:training_time}), the memory footprint for inference is in the order of Mega-Bytes.(\cite{han2016eie}) As a matter of fact, inference is latency sensitive, but computationally less demanding. \cite{8573476}\\



\section{Uncertainty in Deep Neural Networks}
In the previous section we observed that a benchmark should produce predictable results. Although this is true in general terms, we can not properly use the term "predictability" when discussing DNNs. \\
Due to the increased trust given by the advancements in recent years, DNNs are being applied in numerous different fields, as shown by the increasing number of peer review papers about Deep Learning (Fig n. \ref{fig:annual_trend}), even the higher-risk ones 
like medical image analysis or autonomous vehicle control. \cite{gawlikowski2021survey}
\begin{figure}[h]
    \centering
    \includegraphics{img/Annual-trend-in-the-number-of-papers-related-to-deep-learning-in-the-medical-field_W640.jpg}
    \caption[Trend in publications about Deep Learning]{Trend in publications about Deep Learning  \cite{Number_of_DL_papers}}
    \label{fig:annual_trend}
\end{figure}


In such high-risk scenarios, DNNs should not only be highly accurate, as a mistake in detecting an obstacle could cause catastrophic consequences for the passengers, but also be trustworthy enough so that the probability of the predicted values reflects the ground truth. DNNs, however, are subjected to different source of uncertainty and errors. \textit{Galiwoski et al.} in \cite{gawlikowski2021survey} recognize five crucial factors which can cause uncertainty and errors in DNNs.\\\hfill

Those factors are:

\begin{enumerate}[label=\Roman*.]
    \item the variability in real world situations. 
    \item the errors inherent to the measurement systems,
    \item the errors in the architecture specification of the DNN,
    \item the errors in the training procedure of the DNN,
    \item the errors caused by unknown data.
\end{enumerate}
%In the next section, we will explore these factor in more details. Subsequently, we will reason about methods to reduce it and finally, in the third section, we will individuate methods to classify it. 
%\subsection{Factors causing Uncertainty}
We can model a neural network as a function $f$, parametrized by weight $\theta$, which maps a set of input $\mathbb{X}$ to a set of measurable outputs $\mathbb{Y}$, hence:
\begin{equation}
    f\theta : \mathbb{X} \rightarrow \mathbb{Y} \quad\quad\quad
    f\theta(x) = y
    \label{eq:DNN_model}
\end{equation}
In case of supervised learning, we also model the training set as $\mathcal{D} \subset \mathbb{D} = \mathbb{X} \times \mathbb{Y}$, where $\mathbb{X}$ is an instance space and $\mathbb{Y}$ the set of outcomes that can be associated with an instance (\cite{uncertainity_classi}), containing $N$ samples. A DNN trained in $\mathcal{D}$ can therefore predict a a corresponding target $f\theta(x^*) = y^*$.\\
During the data acquisition process, a measurement $x$ and a target variable $y$ are taken from space $\Omega$ to  represent a real world situation $\omega$. For example, $\omega$ could be a sugar beet, $y$ the label ''sugar beet'' and $x$ a picture of a sugar beet. The job of the DNN is therefore to predict the label from the image of the sugar beet (Eq n. \ref{eq:DNN_model}). In a real world situation, however, measurements could be potentially different from the ones used for training and this could influence the prediction of $y$. The source of this difference could be different lighting condition, different environmental condition or general conditions not taken into account when training.A new measurement generally is not part of the training set, hence $x^* \notin \mathcal{D}$.These differences in real world scenarios compared to the training set are called \textit{distribution shifts}(\cite{ovadia2019trust}) and DNN are very sensitive to that. Moreover, source of noise could also be individuated in errors in labelling. This is the first factor that can cause uncertainty, i.e. \textbf{the variability in real world situations}. \cite{gawlikowski2021survey}\\
In addition, it has to be taken into account that measurement devices are also subjected to noise due to defects or imprecision. This kind of noise is the second factor, i.e. \textbf{tthe errors inherent to the measurement systems}. \cite{gawlikowski2021survey}\\
We previously modelled a neural network simply as a function $f\theta$. Although in most applications DNN are treated similarly to how we modelled them, i.e. as a black box which takes as input some data and outputs a prediction, they have a certain structure with certain parameters to take care of. These parameters can be for example the amount of layer, the parameters which need to be configured and the chosen activation function. These configurations are up to the modeller and are the third cause of uncertainty in DNNs, i.e. \textbf{the errors in the architecture specification of the DNN}. \cite{gawlikowski2021survey}\\
A further source of uncertainty in DNNs is also related to the very own nature of DNNs. DNNs are not deterministic, rather they are stochastic\footnote{Stochastic is a synonym for randomness} in many ways : the order of data, the weight initialization or random regularization as augmentation or dropout. \cite{gawlikowski2021survey} Moreover, the stochastic gradient descent algorithm is widely use to optimize the learning process of DNNs. \cite{ruder2017overview}\\
Another version of the gradient descent algorithm which is also used is the mini-batch version, which selects random batches with aleatory size (called batch size) from the learning data-set to optimize the training process. \cite{ruder2017overview} This parameter, in addition to other ones like learning rate, and the number of training epochs, is also source of unpredictability in the DNN, as they could heavily change its performances  \\
The stochastic nature of DNN and the choices of the aforementioned parameters are the forth cause of uncertainty in DNNs : \textbf{the errors in the training procedure of the DNN}\\
Previously we modelled the training set as $\mathcal{D} \subset \mathbb{D}$, hence as being a subset of a certain domain $\mathbb{D}$. However, the realm of input which could be fed to a neural network is not limited to this domain, rather the DNN is trained to solve tasks with inputs belonging to this domain. Another source of uncertainty can be therefore an input which the DNN is not trained for, even though it is able to process it. For instance, if a DNN is trained to classify dogs from images, an image as input could also depict a bird. The last factor which could cause uncertainty is therefore the \textbf{errors caused by unknown data}. Such unknown data, could also be caused by highly noisy measurement devices, like a broken camera, and some researches show how easy is to fool DNN with images that are meaningless for us humans.\cite{nguyen2015deep} Fig n. \ref{fig:fooling_DNN} depicts some examples of picture which could fool even the most advanced DNN. The figures on the top row could also be produced by random noise from a malfunctioning camera and yet could be classified as something else with an accuracy $\geq$ 99.6\%. \cite{nguyen2015deep}\\
\begin{figure}[htb]
    \centering
    \includegraphics[scale = 0.4]{img/images_fooling_DNN.png}
    \caption[Differences between how DNNs and humans recognize objects]{This result highlights differences between how DNNs and humans recognize objects. Images are either directly (top) or indirectly (bottom) encoded \cite{nguyen2015deep}}
    \label{fig:fooling_DNN}
\end{figure}

With the help of the five factors we studied before, we can further categorize uncertainty in two main categories: aleatoric and epistemic uncertainty. \cite{Separation_uncer}

\begin{figure}[htb]
    \includegraphics[scale = 0.4]{img/uncert_visua.png}
    \caption[Illustrating the difference between aleatoric and epistemic uncertainty]{Illustrating the difference between aleatoric and epistemic uncertainty for semantic segmentation on the CamVid dataset \cite{DBLP:journals/corr/abs-1811-01412}}
    \label{fig:un_visua}
\end{figure}




\textit{Aleatoric uncertainty} is also usually referred to as data or statistical uncertainty) and it is generally used to identify the type of uncertainty cause by the nature of randomness, that is \textit{ the variability in the outcome of an experiment which is due to inherently random effects} \cite{Separation_uncer}. Moreover, it also comprises the type of noise directly stemming from the noise in data used to train, validate, test and for inference, therefore referring to factor II \cite{gawlikowski2021survey} \cite{DBLP:journals/corr/abs-1811-01412}.  \\
Aleatoric uncertainty is of the irreducible type, meaning it is impossible to get rid of.(\cite{DBLP:journals/corr/KendallG17}, \cite{Separation_uncer}) For example, a model that predicts the output of a flip coin, even in case of a perfect model, is not able to produce the definite, correct answer for it, but only a prediction of the possibilities for the two outcomes. This is due to the fact that the data used for this model is of random, stochastic nature and this random component can not be reduced. \cite{Separation_uncer}\\
\textit{Epistemic uncertainty}, or systemic or epistemic uncertainty, on the other hand, refers to the type of uncertainty cause by ignorance or short comings of the model, hence not on any other underlying stochastic phenomenon. \cite{DBLP:journals/corr/abs-1811-01412}. Usually, this is due to unknown or lack of coverage of the dataset used for training, hence covers factors I,III, IV and V. \\
Epistemic uncertainty can be theoretically explained away by improving the architecture of the model and broadening the dataset.
For example,let us consider a model which is able to determine whether a word is part of the Italian or English dictionary. If we feed the word 'macchina'' to the model, given that we provided enough data to train and the model is perfect, we can be confident that it will predict the correct language, in this case Italian. However, if we feed it with the word ''pasta'', we can not be confident that the prediction is correct, i.e. we can only obtain the probability of this word to be part of one of the dictionaries \textbf{given the dataset we provided}. As a matter of fact, the word ''pasta'' is present in both dictionaries and the outcome depends on the probability of the word to appear in one of the dictionaries in our dataset. The first case is clear case of epistemic uncertainty, which we were able to completely remove with a perfect dataset; while the second case is a clear case of aleatoric uncertainty and it is impossible to remove. 
Fig n\ref{fig:un_visua} helps visualize the difference between the two. In pictures (\textit{d}), we can see that the aleatoric uncertainty increases on object boundaries and objects far from the camera. Pictures (\textit{e}), on the other hand, shows how epistemic uncertainty increases for semantically and visually challenging pixels. For example, the bottom row shows a failure case when the model fails to segment the footpath due to high epistemic uncertainty, but low aleatoric uncertainty. \cite{DBLP:journals/corr/KendallG17}\\
Although this distinction is very helpful for further and more precise analysis on a DNN model, it is also important to note that the distinction between the two is highly dependent on the context: they are not absolute notions.(\cite{KIUREGHIAN2009105}) Hence, changing the settings of the model will blur the distinction line between the two and therefore make their classification considerably more difficult.\cite{Separation_uncer}\\
We mentioned previously that theoretically model uncertainty is 100\% reducible. However, in case of real world data this is not the case. In addition to the probabilistic nature of the DNNs, the training dataset used for the model is very likely to be only a subset of all possible input data of the application, hence it is also very likely that unknown data for the domain is unavoidable. In addition, represent exactly the uncertainty of the DNN is not possible, as the different sources of uncertainty can not be generally modelled accurately. \cite{gawlikowski2021survey}\\

%\subsubsection{Modelling Uncertainty}
%\subsection{Classification of uncertainty}

%\subsection{Quantification of uncertainty}

%For a desired degree of confidence (namely, foir a given probability), a confidence interval is a prediction of the range of the output of a model where the actual value exists. With an assumption of a normal distribution of the errors, confidence intervals can be calculated for neural networks.\cite{Confidence_Interval}
%\subsection{Calibration of Neural Networks}
%\subsection{Final remarks on Uncertainty}


\section{Accuracy}\label{sec:accuracy}
Accuracy is usually one of the most looked after metrics when analysing Neural Network performances(\cite{hendrycks2019benchmarking},\cite{bianco2018dnnsbench}). Accuracy is usually the factor taken into account when the training process is evaluated, as could be a potential factor deciding when to stop it. Being able to predict accuracy, for example, could help detecting, and therefore terminate, unsuccessful training run \cite{unterthiner2021predicting}. \\
 Usually, the term ''accuracy''  refers to the \textit{classification accuracy} of the neural network. The classification accuracy is the ratio of the number of correct predictions to the total number of input sample (\cite{hussein}) and it is calculated with equation n. \ref{eq:cla_acc}.
\begin{equation}
Accuracy = \dfrac{Number\;of\;correct\;predictions}{Total\;number\;of\;predictions}
\label{eq:cla_acc}    
\end{equation}

In case of a multi-class prediction problem, however, classification accuracy is meaningful only when each class contains an equal number of samples. For example, considering two classes $A$ and $B$ containing 98\% and 2\% of the samples respectively, the model can reach 98\% accuracy by predicting exclusively samples from class $A$. \\
Another definition which is often used for accuracy in Neural Networks in the context of \textit{binary classification} is the one showed in equation n. \ref{eq:bin_acc}.
 \begin{equation}
Accuracy = \dfrac{TP+TN}{TP+TN+FP+FN}
\label{eq:bin_acc}    
\end{equation}
Where TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.\\
Similarly to classification accuracy, however, this definition could be misleading as well.  For example, considering a model that classified 100 tumors as malignant (the positive class) or benign (the negative class) as shown in Fig. n. \ref{fig:tumor}. 

\begin{figure}[h]
\centering
\begin{tabular}{ p{1cm} p{2cm} p{2cm} p{2cm}}
 TP&FP&FN&TN\\
 \hline
    1 & 1& 8& 90\\
\end{tabular}
\caption{Example for binary classification}
\label{fig:tumor}
\end{figure}
Calculating the accuracy using equation n. \ref{eq:bin_acc}. will give an accuracy of :
 \begin{equation}
Accuracy = \dfrac{1+90}{1+90+1+80} = 0.91
\label{eq:bin_acc2}    
  \end{equation}
At first glance, this metric would show that the model is performing correctly (91 correct out of a 100). The dataset is composed of 91 tumors that are benign (90 TNs and 1 FP) and 9 that are malignant (1 TP and 8 FNs). The model is able to identify 90 out of 91 benign tumors correctly, but only 1 out of 9 as malignant. This is another case of class-imbalanced data set in which a model that would only classify tumors as benign would reach the same level of accuracy. \cite{google_doc}\\



\section{Precision and Recall}
As we already delineated in the previous section, sometimes analysing only at accuracy might be misleading, especially when we are dealing with class-imbalanced data-sets. Accuracy does not distinguish between the number of correct labels of different classes. \cite{metrics}\\
In these cases, two other metrics we can observe are \textit{precision} and \textit{recall}. \\
Formally, \textit{precision} is defined as equation n. \ref{eq:pre} and it represents the number of detected anomalies that are actual real anomalies.\cite{tatbul2019precision}
In other words, it expresses the proportion of positive identifications that was correct. 
\begin{equation}
Precision = \dfrac{TP}{TP+FP}
\label{eq:pre}    
\end{equation}
Where TP = True Positives and FP = False Positives\\
On the other hand, \textit{recall} is defined as equation n. \ref{eq:rec} and it represents the number of real anomalies that have been detected.\cite{tatbul2019precision}
\begin{equation}
Recall = \dfrac{TP}{TP+FN}
\label{eq:rec}    
\end{equation}
Where TP = True Positives and FN = False Negatives\\

We can use the example in Fig. n . \ref{fig:tumor} to calculate both precision and recall. 
\begin{equation}
\begin{aligned}
Precision &= \dfrac{TP}{TP+FP}\\
          &= \dfrac{1}{1+1}\\
          &= \dfrac{1}{2}\\
          &= 0.5
\end{aligned}
\label{eq:pre_ex}    
\end{equation}
Having a precision of 0.5, our model is correct 50\% of the time when it predicts that a tumor is malignant. 
\begin{equation}
\begin{aligned}
Recall &= \dfrac{TP}{TP+FN}\\
          &= \dfrac{1}{1+8}\\
          &= \dfrac{1}{9}\\
          &= 0.11
\end{aligned}
\label{eq:rec2}    
\end{equation}
In other words, with a recall of 0.11, our model has been able to identify 11\% of all malignant tumors. \\
Ideally, we would like to have a high recall percentage as well as a high precision one. Unfortunately, this is rarely the case since improving recall often comes at the expense of precision, since in order to increase the TP for the minority class, the number of FP is also often increased, resulting in reduced precision. \cite{10.5555/2559492}\\
Furthermore, as it was the case for accuracy, neither precision nor recall give a full insight on the performance of the model, since a model could have high precision with low recall, as shown in the example above. It is, therefore, common practice to combine both with a weighted harmonic mean as an F score \cite{Rijsbergen1974FOUNDATIONOE}:
\begin{equation}
F_\beta = (1+ \beta ^2)\dfrac{Precision * Recall}{\beta ^2* Precision + Recall}
\label{eq:f_score}    
\end{equation}
In equation n. \ref{eq:f_score}, the coefficient $\beta$ represents the balance between Precision and Recall, with high values favouring Recall. Usually, the F-score is used with $\beta =1$, so a perfect balance between Precision and Recall, and in this case it is called F1 score(Equation n. \ref{eq:f1_score}). \cite{derczynski-2016-complementarity}
\begin{equation}
F1 = (2)\dfrac{Precision * Recall}{Precision + Recall}
\label{eq:f1_score}    
\end{equation}
When evaluating the F score, however,the focus is given exclusively to true positives, false positives and false negatives, neglecting the true negative group, which is usually the majority class. In addition, using the F score, one is unable to distinguish low-recall from low-precision systems. \cite{derczynski-2016-complementarity}\\
\section{Loss}
When training a Neural Network, finding the perfect weights for the neurons is impossible, therefore the problem is usually modelled as an optimization problem. As an optimization problem, it is solved using an algorithm which aims to optimize the weights to make good predictions. Usually, Neural Networks are trained using the Stochastic Gradient Descent (SGD) and the weights are updated through back-propagation. In the context of an optimization algorithm, the function which evaluates a candidate solution is defined as the objective function and such function in Neural Networks evaluates how good a prediction is, hence the SGD is used to minimize this function, i.e. finding the solution with the lowest score. 
When we are minimizing the objective function, we are referring to it as the cost function, loss function, or error function.\cite{Goodfellow-et-al-2016}\\
The loss function has the fundamental job of faithfully distill all the aspects of the model, both good and bad, down into a single, scalar value, which allows candidate solutions to be compared. \cite{reed_neural_1999}\\
The choice of the loss function is highly influenced by the output layer of the Neural Network. The output layer defines the type of the solution we defined for the problem, i.e. if it is a regression problem of a classification problem. In other words, the way we  represent the output determines the loss function. \cite{Goodfellow-et-al-2016}\\
The following are some of the best practices for each type of problem. 

\begin{description}
  \item[Regression problem] \hfill\\ 
  For these type of problems, usually the output layer is one node with a linear activation unit, therefore the loss function to use is the Mean Squared Error(MSE). 
  \item[Binary Classification problem] \hfill\\ 
  The output layer is one node with a sigmoid activation unit and the loss function used is Cross-Entropy, or Logarithmic loss. 
  \item[Multi-class Classification problem] \hfill\\ 
  The output layer is composed of one node for each class using the soft-max activation function nd the loss function used is Cross-Entropy, or Logarithmic loss. 
\end{description}


%\cite{10.5555/525960}



%\url{https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/}

\section{Over-fitting and Under-fitting }\label{sec:of_uf}
As we already discovered in section n. \ref{sec:training_time}, the training routine is necessary for the model in order to make predictions on a new set of data. Therefore, the goal of the learning process is not to minimize the accuracy on the training set, but rather to maximize its predictive accuracy on the new data points. \citep{dietterich1995overfitting}\\
If the model works too hard to find the best fit for the training data there is the risk that it is going to fit the noise present on the data-set, hence it will learn the inevitable peculiarities and bias present in the training set, rather than finding a general predictive rule. In other words, the performance of the model will decrease when tested against an unknown data-set.\cite{jabbar2015methods} This phenomena is called \textit{over-fitting}. \citep{dietterich1995overfitting}\\
Over-fitting is influenced by the training data fed to the model, as small data-sets are more prone to it, even though also large data-sets can be affected. \cite{10.1016/j.inffus.2008.11.003}\\
Under-fitting, on the other hand, can be considered as the opposite process. This occurs when the model is too simple for the training data and it is incapable of learning the variability of the data. \cite{10.1016/j.inffus.2008.11.003}\\

One strategy to avoid over-fitting is to use a so-called ''early-stopping'' approach. This approach consists simply of stopping the training before over-fitting can occur and it is one of the mostly used strategies to avoid this phenomena, as it is simple to understand and proven to be the superior one in many cases (\citep{FINNOFF1993771}, \cite{early_stopping}). \\
The key part of this approach is to divide the training set into two parts, namely the validation set and the training set, and to use those to find a criteria to stop the training process. \\
Fig. n. \ref{fig:over_fitting_curve} shows the idealized evolution over time of the error on the training set and on a validation set not used for training, i.e. training and validation loss.\cite{early_stopping}

\begin{figure}[htb]
    \centering
    \includegraphics[scale = 0.3]{img/over_fitting.png}
    \caption[Validation and training loss curve]{Validation and training loss curve. The x axis is time, the y axis is the loss \cite{early_stopping}}
    \label{fig:over_fitting_curve}
\end{figure}


Following the trend in Fig. n. \ref{fig:over_fitting_curve}, we can observe that the validation error decreases until a certain point together with the training loss before increasing indefinitely. We can use the validation error to approximate the generalization error and, therefore, stop the training in the moment where the validation loss is increasing again. 
We can summarize the ''early-stopping'' approach in the following steps:
\begin{enumerate}
\item Split the training data into a training set and a validation set, for i.g. in a 4-to-1 proportion
\item Train over the training and evaluate the validation loss every arbitrary number of epoch
\item Stop training as so on as the error on the validation set is         higher than it was last time
\item Use the weights the network had in that previous step as the result of the training run

\end{enumerate}
Furthermore, depending on the data, there might be also situations in which the validation loss is not constantly bigger than the training loss, as shown in Fig. n. \ref{fig:over_fitting_curve}. As a matter of fact, this graph shows an idealized behaviour of the two curves. A real example for this curve can be seen in Fig. n. \ref{fig:over_fitting_curve_real}.
\begin{figure}[htb]
    \centering
    \includegraphics[scale = 0.4]{img/over_fitting_real.png}
    \caption[Real Example of the Validation and training loss curve]{Real Example of the Validation and training loss curve. The x axis is time, the y axis is the loss \cite{early_stopping}}
    \label{fig:over_fitting_curve_real}
\end{figure}

In such situations, it would be particularly difficult to choose an optimal stopping criteria, as the validation function as multiple local minima (16 in the pictures case). It is left to the reader to investigate strategies to optimize early-stopping in these situations. \\
Furthermore, there are also scenarios in which, depending on the data presented, the validation loss is at first less than the training loss, as shown in Fig. n. \ref{fig:over_fitting_curve_2}. 
\begin{figure}[htb]
    \centering
    \includegraphics[scale = 0.4]{img/over_fitting_2.png}
    \caption[Real Example of the Validation and training loss curve]{Real Example of Validation and training loss curve. The x axis is time, the y axis is the loss}
    \label{fig:over_fitting_curve_2}
\end{figure}

In these situation, as a rule of thumb, we can use the following statements to determine the status of the training and decide when to stop. 
\begin{itemize}
\item validation loss $>>$ training loss: over-fitting
\item validation loss $>$ training loss: some over-fitting
\item validation loss $<$ training loss: some under-fitting
\item validation loss $<<$ training loss: under-fitting

\end{itemize}


\section{Architecture}
\subsection{Alexnet}