\chapter{Analysis of the characteristics}\label{ana_char}
In this chapter, we will analyse and measure different characteristics of various Neural Network architectures, with the purpose of finding useful correlations which can be used in a later stage. In order to achieve this goal, we will use our understanding from chapter \ref{char_nn} of each metric of Neural Network and the tool we developed in section \ref{sec:my_bench}. \\
In challenges such as the ImageNet classification challenge (\cite{ILSVRC15}) the ultimate goal is to achieve the highest accuracy possible, neglecting other performance metrics like inference time. \cite{DBLP:journals/corr/CanzianiPC16}\\
Although accuracy is of high importance, in practical applications other metrics are to be considered as well, depending on the different requirements. As pointed out by \textit{Canziani et al.} in \cite{DBLP:journals/corr/CanzianiPC16}, metrics like inference time, parameters, and operations count are hard constraints
for the deployment of Neural Networks in practical applications. Furthermore, training time is often a time consuming process which highly depends on factors like the complexity of the task, size of the network, and training set(\cite{118273}). \\
Finding relations between these metrics and other factors as well, like influence of a given input feature to the prediction of the model (\cite{hooker2019benchmark}), will allow us to optimise applications, saving time and resources in the process.
\section{Test Environment}
All the experiments were run on the same machine running Ubuntu 20.04.3 LTS (Focal Fossa). For the specific of the machine, please refer to table \ref{tab:gpu_info}
\begin{table}[h]
\centering
\begin{tabular}{|l  |r|}
 \hline
CPU & AMD EPYC 7452 32-Core Processor\\
CPU MHz&                     1499.324\\
CPU max MHz&                     2350,0000\\
CPU min MHz&                     1500,0000\\
Total memory&       1056709772 kB\\
GPU&    Nvidia A100-PCIE-40GB\\
Number of GPUs & 8\\
\hline
\end{tabular}
\caption{Specifics of the machine which run the experiments}
\label{tab:gpu_info}
\end{table}

It is assumed for all the use cases that, if no specification is made, the training of the models has been carried out by the \textit{fit\_one\_cycle()} function present in fastai using the default learning rate. Furthermore, the models have not been pre-trained, hence no transferred learning is applied, and the models have been trained using full precision. \\







\input{src/First_Experiment/plants}
\input{src/Second_Experiment/Pets}

