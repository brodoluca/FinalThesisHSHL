@article{ayaz_internet--things_2019,
	title = {Internet-of-{Things} ({IoT})-{Based} {Smart} {Agriculture}: {Toward} {Making} the {Fields} {Talk}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {Internet-of-{Things} ({IoT})-{Based} {Smart} {Agriculture}},
	url = {https://ieeexplore.ieee.org/document/8784034/},
	doi = {10.1109/ACCESS.2019.2932609},
	urldate = {2021-08-20},
	journal = {IEEE Access},
	author = {Ayaz, Muhammad and Ammad-Uddin, Mohammad and Sharif, Zubair and Mansour, Ali and Aggoune, El-Hadi M.},
	year = {2019},
	pages = {129551--129583},
	file = {Full Text:/Users/brodie/Zotero/storage/UGFYYHFS/Ayaz et al. - 2019 - Internet-of-Things (IoT)-Based Smart Agriculture .pdf:application/pdf},
}
@article{bhadra_weed_2020,
	title = {Weed management in sugar beet: {A} review},
	volume = {5},
	issn = {2518-2021},
	shorttitle = {Weed management in sugar beet},
	url = {https://www.ejmanager.com/fulltextpdf.php?mno=83758},
	doi = {10.5455/faa.83758},
	number = {0},
	urldate = {2021-08-18},
	journal = {Fundamental and Applied Agriculture},
	author = {Bhadra, Tamalika and Paul, Swapan},
	year = {2020},
	pages = {1},
	file = {Full Text:/Users/brodie/Zotero/storage/EC52NVIS/Bhadra e Paul - 2020 - Weed management in sugar beet A review.pdf:application/pdf},
}
@article{schweizer_weed_1989,
	title = {"{Weed} control in sugarbeets ({Beta} vulgaris) in {North} {America}"},
	author = {Schweizer, EE and Dexter, A.G},
	year = {1989},
}
@article{may_economic_2003,
	title = {Economic consequences for {UK} farmers of growing {GM} herbicide tolerant sugar beet},
	volume = {142},
	issn = {0003-4746, 1744-7348},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1744-7348.2003.tb00227.x},
	doi = {10.1111/j.1744-7348.2003.tb00227.x},
	language = {en},
	number = {1},
	urldate = {2021-08-20},
	journal = {Annals of Applied Biology},
	author = {May, M J},
	month = feb,
	year = {2003},
	pages = {41--48},
}
@article{singh_machine_2016,
	title = {Machine {Learning} for {High}-{Throughput} {Stress} {Phenotyping} in {Plants}},
	volume = {21},
	issn = {1360-1385},
	url = {https://www.sciencedirect.com/science/article/pii/S1360138515002630},
	doi = {https://doi.org/10.1016/j.tplants.2015.10.015},
	abstract = {Advances in automated and high-throughput imaging technologies have resulted in a deluge of high-resolution images and sensor data of plants. However, extracting patterns and features from this large corpus of data requires the use of machine learning (ML) tools to enable data assimilation and feature identification for stress phenotyping. Four stages of the decision cycle in plant stress phenotyping and plant breeding activities where different ML approaches can be deployed are (i) identification, (ii) classification, (iii) quantification, and (iv) prediction (ICQP). We provide here a comprehensive overview and user-friendly taxonomy of ML tools to enable the plant community to correctly and easily apply the appropriate ML tools and best-practice guidelines for various biotic and abiotic stress traits.},
	number = {2},
	journal = {Trends in Plant Science},
	author = {Singh, Arti and Ganapathysubramanian, Baskar and Singh, Asheesh Kumar and Sarkar, Soumik},
	year = {2016},
	keywords = {abiotic stress, biotic stress, high-throughput phenotyping, Imaging, machine learning, plant breeding},
	pages = {110--124},
}
@misc{masclef_sugar_1891,
	title = {Sugar {Beet}},
	url = {https://commons.wikimedia.org/w/index.php?curid=5767279},
	author = {Masclef, Amédée},
	year = {1891},
}
@inproceedings{lottes_effective_2016,
	address = {Stockholm, Sweden},
	title = {An effective classification system for separating sugar beets and weeds for precision farming applications},
	isbn = {978-1-4673-8026-3},
	url = {http://ieeexplore.ieee.org/document/7487720/},
	doi = {10.1109/ICRA.2016.7487720},
	urldate = {2021-08-16},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	publisher = {IEEE},
	author = {Lottes, P. and Hoeferlin, M. and Sander, S. and Muter, M. and Schulze, P. and Stachniss, Lammers C.},
	month = may,
	year = {2016},
	pages = {5157--5163},
	annote = {Weed identification using mobile robot
USes RF, howver since the robot is very close to the plants, they are able to identify the vegetation using a special type of camera.},
}

@article{islam_review_2021,
	title = {A {Review} of {Applications} and {Communication} {Technologies} for {Internet} of {Things} ({IoT}) and {Unmanned} {Aerial} {Vehicle} ({UAV}) {Based} {Sustainable} {Smart} {Farming}},
	volume = {13},
	issn = {2071-1050},
	url = {https://www.mdpi.com/2071-1050/13/4/1821},
	doi = {10.3390/su13041821},
	abstract = {To reach the goal of sustainable agriculture, smart farming is taking advantage of the Unmanned Aerial Vehicles (UAVs) and Internet of Things (IoT) paradigm. These smart farms are designed to be run by interconnected devices and vehicles. Some enormous potentials can be achieved by the integration of different IoT technologies to achieve automated operations with minimum supervision. This paper outlines some major applications of IoT and UAV in smart farming, explores the communication technologies, network functionalities and connectivity requirements for Smart farming. The connectivity limitations of smart agriculture and it’s solutions are analysed with two case studies. In case study-1, we propose and evaluate meshed Long Range Wide Area Network (LoRaWAN) gateways to address connectivity limitations of Smart Farming. While in case study-2, we explore satellite communication systems to provide connectivity to smart farms in remote areas of Australia. Finally, we conclude the paper by identifying future research challenges on this topic and outlining directions to address those challenges.},
	language = {en},
	number = {4},
	urldate = {2021-08-16},
	journal = {Sustainability},
	author = {Islam, Nahina and Rashid, Md Mamunur and Pasandideh, Faezeh and Ray, Biplob and Moore, Steven and Kadel, Rajan},
	month = feb,
	year = {2021},
	pages = {1821},
	annote = {Applications of technologies in smart farming.
It's a general overview},
	file = {Full Text:/Users/brodie/Zotero/storage/8HD6EJ8M/Islam et al. - 2021 - A Review of Applications and Communication Technol.pdf:application/pdf},
}

@article{glaroudis_survey_2020,
	title = {Survey, comparison and research challenges of {IoT} application protocols for smart farming},
	volume = {168},
	issn = {13891286},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389128619306942},
	doi = {10.1016/j.comnet.2019.107037},
	language = {en},
	urldate = {2021-08-20},
	journal = {Computer Networks},
	author = {Glaroudis, Dimitrios and Iossifides, Athanasios and Chatzimisios, Periklis},
	month = feb,
	year = {2020},
	pages = {107037},
}





@article{Fleming1986HowNT,
  title={How not to lie with statistics: the correct way to summarize benchmark results},
  author={Philip J. Fleming and John J. Wallace},
  journal={Commun. ACM},
  year={1986},
  volume={29},
  pages={218-221}
}


@article{DBLP:journals/corr/abs-1811-01412,
  author    = {Martin Becker and
               Samarjit Chakraborty},
  title     = {Measuring Software Performance on Linux},
  journal   = {CoRR},
  volume    = {abs/1811.01412},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.01412},
  eprinttype = {arXiv},
  eprint    = {1811.01412},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-01412.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{Real-Time-Systems,
  author={Wägemann, Peter and Distler, Tobias and Eichler, Christian and Schröder-Preikschat, Wolfgang},
  booktitle={2017 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)}, 
  title={Benchmark Generation for Timing Analysis}, 
  year={2017},
  volume={},
  number={},
  pages={319-330},
  doi={10.1109/RTAS.2017.6}}



@inproceedings{how_to_bench,
author = {von Kistowski, Jóakim and Arnold, Jeremy and Huppler, Karl and Lange, Klaus-Dieter and Henning, John and Cao, Paul},
year = {2015},
month = {02},
pages = {},
title = {How to Build a Benchmark},
journal = {ICPE 2015 - Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
doi = {10.1145/2668930.2688819}
}


@inproceedings{Stewart2001MeasuringET,
  title={Measuring Execution Time and Real-Time Performance},
  author={David B. Stewart},
  year={2001}
}



@book{linux_commands,
author = {Kerrisk, Michael},
title = {The Linux Programming Interface: A Linux and UNIX System Programming Handbook},
year = {2010},
isbn = {1593272200},
publisher = {No Starch Press},
address = {USA},
edition = {1st},
abstract = {The Linux Programming Interface is the definitive guide to the Linux and UNIX programming
interfacethe interface employed by nearly every application that runs on a Linux or
UNIX system. In this authoritative work, Linux programming expert Michael Kerrisk
provides detailed descriptions of the system calls and library functions that you
need in order to master the craft of system programming, and accompanies his explanations
with clear, complete example programs. You'll find descriptions of over 500 system
calls and library functions, and more than 200 example programs, 88 tables, and 115
diagrams. You'll learn how to: Read and write files efficiently Use signals, clocks,
and timers Create processes and execute programs Write secure programs Write multithreaded
programs using POSIX threads Build and use shared libraries Perform interprocess communication
using pipes, message queues, shared memory, and semaphores Write network applications
with the sockets API While The Linux Programming Interface covers a wealth of Linux-specific
features, including epoll, inotify, and the /proc file system, its emphasis on UNIX
standards (POSIX.1-2001/SUSv3 and POSIX.1-2008/SUSv4) makes it equally valuable to
programmers working on other UNIX platforms. The Linux Programming Interface is the
most comprehensive single-volume work on the Linux and UNIX programming interface,
and a book that's destined to become a new classic. Praise for The Linux Programming
Interface "If I had to choose a single book to sit next to my machine when writing
software for Linux, this would be it." Martin Landers, Software Engineer, Google "This
book, with its detailed descriptions and examples, contains everything you need to
understand the details and nuances of the low-level programming APIs in Linux . .
. no matter what the level of reader, there will be something to be learnt from this
book." Mel Gorman, Author of Understanding the Linux Virtual Memory Manager "Michael
Kerrisk has not only written a great book about Linux programming and how it relates
to various standards, but has also taken care that bugs he noticed got fixed and the
man pages were (greatly) improved. In all three ways, he has made Linux programming
easier. The in-depth treatment of topics in The Linux Programming Interface . . .
makes it a must-have reference for both new and experienced Linux programmers." Andreas
Jaeger, Program Manager, openSUSE, Novell "Michael's inexhaustible determination to
get his information right, and to express it clearly and concisely, has resulted in
a strong reference source for programmers. While this work is targeted at Linux programmers,
it will be of value to any programmer working in the UNIX/POSIX ecosystem." David
Butenhof, Author of Programming with POSIX Threads and Contributor to the POSIX and
UNIX Standards ". . . a very thoroughyet easy to readexplanation of UNIX system and
network programming, with an emphasis on Linux systems. It's certainly a book I'd
recommend to anybody wanting to get into UNIX programming (in general) or to experienced
UNIX programmers wanting to know 'what's new' in the popular GNU/Linux system." Fernando
Gont, Network Security Researcher, IETF Participant, and RFC Author ". . . encyclopedic
in the breadth and depth of its coverage, and textbook-like in its wealth of worked
examples and exercises. Each topic is clearly and comprehensively covered, from theory
to hands-on working code. Professionals, students, educators, this is the Linux/UNIX
reference that you have been waiting for." Anthony Robins, Associate Professor of
Computer Science, The University of Otago "I've been very impressed by the precision,
the quality and the level of detail Michael Kerrisk put in his book. He is a great
expert of Linux system calls and lets us share his knowledge and understanding of
the Linux APIs." Christophe Blaess, Author of Programmation systeme en C sous Linux
". . . an essential resource for the serious or professional Linux and UNIX systems
programmer. Michael Kerrisk covers the use of all the key APIs across both the Linux
and UNIX system interfaces with clear descriptions and tutorial examples and stresses
the importance and benefits of following standards such as the Single UNIX Specification
and POSIX 1003.1." Andrew Josey, Director, Standards, The Open Group, and Chair of
the POSIX 1003.1 Working Group "What could be better than an encyclopedic reference
to the Linux system, from the standpoint of the system programmer, written by none
other than the maintainer of the man pages himself? The Linux Programming Interface
is comprehensive and detailed. I firmly expect it to become an indispensable addition
to my programming bookshelf." Bill Gallmeister, Author of POSIX.4 Programmer's Guide:
Programming for the Real World ". . . the most complete and up-to-date book about
Linux and UNIX system programming. If you're new to Linux system programming, if you're
a UNIX veteran focused on portability while interested in learning the Linux way,
or if you're simply looking for an excellent reference about the Linux programming
interface, then Michael Kerrisk's book is definitely the companion you want on your
bookshelf." Loic Domaigne, Chief Software Architect (Embedded), Corpuls.com}
}


@article{Beyer2017ReliableBR,
  title={Reliable benchmarking: requirements and solutions},
  author={Dirk Beyer and Stefan L{\"o}we and Philipp Wendler},
  journal={International Journal on Software Tools for Technology Transfer},
  year={2017},
  volume={21},
  pages={1-29}
}


@online{LinuxManualWeb,
  author = {Michael Kerrisk},
  title = {Linux Manual Page},
  year = 2021,
  url = {https://man7.org/linux/man-pages/man7/cgroups.7.html},
  urldate = {2021-11-04}
}



%%%%%%%%%%%
%Benchmarking Neural Networks

@ARTICLE{Confidence_Interval,
  author={Chryssolouris, G. and Lee, M. and Ramsey, A.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Confidence interval prediction for neural network models}, 
  year={1996},
  volume={7},
  number={1},
  pages={229-232},
  doi={10.1109/72.478409}}



@misc{gawlikowski2021survey,
      title={A Survey of Uncertainty in Deep Neural Networks}, 
      author={Jakob Gawlikowski and Cedrique Rovile Njieutcheu Tassi and Mohsin Ali and Jongseok Lee and Matthias Humt and Jianxiang Feng and Anna Kruspe and Rudolph Triebel and Peter Jung and Ribana Roscher and Muhammad Shahzad and Wen Yang and Richard Bamler and Xiao Xiang Zhu},
      year={2021},
      eprint={2107.03342},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{zhang2021ai,
      title={The AI Index 2021 Annual Report}, 
      author={Daniel Zhang and Saurabh Mishra and Erik Brynjolfsson and John Etchemendy and Deep Ganguli and Barbara Grosz and Terah Lyons and James Manyika and Juan Carlos Niebles and Michael Sellitto and Yoav Shoham and Jack Clark and Raymond Perrault},
      year={2021},
      eprint={2103.06312},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}


@article{Number_of_DL_papers,
author = {Song, Kyoung and Kim, Myeongchan and Do, Synho},
year = {2019},
month = {03},
pages = {202},
title = {The Latest Trends in the Use of Deep Learning in Radiology Illustrated Through the Stages of Deep Learning Algorithm Development},
volume = {80},
journal = {Journal of the Korean Society of Radiology},
doi = {10.3348/jksr.2019.80.2.202}
}

@misc{ovadia2019trust,
      title={Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift}, 
      author={Yaniv Ovadia and Emily Fertig and Jie Ren and Zachary Nado and D Sculley and Sebastian Nowozin and Joshua V. Dillon and Balaji Lakshminarayanan and Jasper Snoek},
      year={2019},
      eprint={1906.02530},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@misc{ruder2017overview,
      title={An overview of gradient descent optimization algorithms}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1609.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{nguyen2015deep,
      title={Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}, 
      author={Anh Nguyen and Jason Yosinski and Jeff Clune},
      year={2015},
      eprint={1412.1897},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}



@INPROCEEDINGS{Separation_uncer,
  author={Huseljic, Denis and Sick, Bernhard and Herde, Marek and Kottke, Daniel},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Separation of Aleatoric and Epistemic Uncertainty in Deterministic Deep Neural Networks}, 
  year={2021},
  volume={},
  number={},
  pages={9172-9179},
  doi={10.1109/ICPR48806.2021.9412616}}
@article{uncertainity_classi,
author = {Hüllermeier, Eyke and Waegeman, Willem},
year = {2021},
month = {03},
pages = {},
title = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
volume = {110},
journal = {Machine Learning},
doi = {10.1007/s10994-021-05946-3}
}

@article{DBLP:journals/corr/KendallG17,
  author    = {Alex Kendall and
               Yarin Gal},
  title     = {What Uncertainties Do We Need in Bayesian Deep Learning for Computer
               Vision?},
  journal   = {CoRR},
  volume    = {abs/1703.04977},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.04977},
  eprinttype = {arXiv},
  eprint    = {1703.04977},
  timestamp = {Mon, 13 Aug 2018 16:48:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KendallG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{KIUREGHIAN2009105,
title = {Aleatory or epistemic? Does it matter?},
journal = {Structural Safety},
volume = {31},
number = {2},
pages = {105-112},
year = {2009},
note = {Risk Acceptance and Risk Communication},
issn = {0167-4730},
doi = {https://doi.org/10.1016/j.strusafe.2008.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167473008000556},
author = {Armen Der Kiureghian and Ove Ditlevsen},
keywords = {Aleatory, Epistemic, Ergodicity, Parameter uncertainty, Predictive models, Probability distribution choice, Statistical dependence, Systems, Time-variant reliability, Uncertainty},
abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems.}
}

@article{bianco2018dnnsbench,
 author = {Bianco, Simone and Cadene, Remi and Celona, Luigi and Napoletano, Paolo},
 year = {2018},
 title = {Benchmark Analysis of Representative Deep Neural Network Architectures},
 journal = {IEEE Access},
 volume = {6},
 pages = {64270-64277},
 doi = {10.1109/ACCESS.2018.2877890},
 ISSN = {2169-3536},
}
@misc{luo2020comparison,
      title={Comparison and Benchmarking of AI Models and Frameworks on Mobile Devices}, 
      author={Chunjie Luo and Xiwen He and Jianfeng Zhan and Lei Wang and Wanling Gao and Jiahui Dai},
      year={2020},
      eprint={2005.05085},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{reddi2020mlperf,
      title={MLPerf Inference Benchmark}, 
      author={Vijay Janapa Reddi and Christine Cheng and David Kanter and Peter Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody Coleman and Sam Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and J. Scott Gardner and Itay Hubara and Sachin Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and Colin Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem Wu and Lingjie Xu and Koichi Yamada and Bing Yu and George Yuan and Aaron Zhong and Peizhao Zhang and Yuchen Zhou},
      year={2020},
      eprint={1911.02549},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{ignatov2019ai,
      title={AI Benchmark: All About Deep Learning on Smartphones in 2019}, 
      author={Andrey Ignatov and Radu Timofte and Andrei Kulik and Seungsoo Yang and Ke Wang and Felix Baum and Max Wu and Lirong Xu and Luc Van Gool},
      year={2019},
      eprint={1910.06663},
      archivePrefix={arXiv},
      primaryClass={cs.PF}
}
@inproceedings{Coleman2017DAWNBenchA,
  title={DAWNBench : An End-to-End Deep Learning Benchmark and Competition},
  author={Cody A. Coleman and Deepak Narayanan and Daniel Kang and Tian Zhao and Jian Zhang and Luigi Nardi and Peter Bailis and Kunle Olukotun and Christopher R{\'e} and Matei A. Zaharia},
  year={2017}
}

@misc{unterthiner2021predicting,
      title={Predicting Neural Network Accuracy from Weights}, 
      author={Thomas Unterthiner and Daniel Keysers and Sylvain Gelly and Olivier Bousquet and Ilya Tolstikhin},
      year={2021},
      eprint={2002.11448},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{OH20041311,
title = {GPU implementation of neural networks},
journal = {Pattern Recognition},
volume = {37},
number = {6},
pages = {1311-1314},
year = {2004},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2004.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0031320304000524},
author = {Kyoung-Su Oh and Keechul Jung},
keywords = {Graphics processing unit(GPU), Neural network(NN), Multi-layer perceptron, Text detection},
abstract = {Graphics processing unit (GPU) is used for a faster artificial neural network. It is used to implement the matrix multiplication of a neural network to enhance the time performance of a text detection system. Preliminary results produced a 20-fold performance enhancement using an ATI RADEON 9700 PRO board. The parallelism of a GPU is fully utilized by accumulating a lot of input feature vectors and weight vectors, then converting the many inner-product operations into one matrix operation. Further research areas include benchmarking the performance with various hardware and GPU-aware learning algorithms.}
}

@article{DBLP:journals/corr/CanzianiPC16,
  author    = {Alfredo Canziani and
               Adam Paszke and
               Eugenio Culurciello},
  title     = {An Analysis of Deep Neural Network Models for Practical Applications},
  journal   = {CoRR},
  volume    = {abs/1605.07678},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.07678},
  eprinttype = {arXiv},
  eprint    = {1605.07678},
  timestamp = {Mon, 13 Aug 2018 16:46:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/CanzianiPC16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10.1007/978-3-642-04274-4_39,
author="Guzhva, Alexander
and Dolenko, Sergey
and Persiantsev, Igor",
editor="Alippi, Cesare
and Polycarpou, Marios
and Panayiotou, Christos
and Ellinas, Georgios",
title="Multifold Acceleration of Neural Network Computations Using GPU",
booktitle="Artificial Neural Networks -- ICANN 2009",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="373--380",
abstract="With emergence of graphics processing units (GPU) of the latest generation, it became possible to undertake neural network based computations using GPU on serially produced video display adapters. In this study, NVIDIA CUDA technology has been used to implement standard back-propagation algorithm for training multiple perceptrons simultaneously on GPU. For the problem considered, GPU-based implementation (on NVIDIA GTX 260 GPU) has lead to a 50x speed increase compared to a highly optimized CPU-based computer program, and more than 150x compared to a commercially available CPU-based software (NeuroShell 2) (AMD Athlon 64 Dual core 6000+ processor).",
isbn="978-3-642-04274-4"
}

@inproceedings{10.1145/3089801.3089804, author = {Cao, Qingqing and Balasubramanian, Niranjan and Balasubramanian, Aruna}, title = {MobiRNN: Efficient Recurrent Neural Network Execution on Mobile GPU}, year = {2017}, isbn = {9781450349628}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3089801.3089804}, doi = {10.1145/3089801.3089804}, abstract = {In this paper, we explore optimizations to run Recurrent Neural Network (RNN) models locally on mobile devices. RNN models are widely used for Natural Language Processing, Machine translation, and other tasks. However, existing mobile applications that use RNN models do so on the cloud. To address privacy and efficiency concerns, we show how RNN models can be run locally on mobile devices. Existing work on porting deep learning models to mobile devices focus on Convolution Neural Networks (CNNs) and cannot be applied directly to RNN models. In response, we present MobiRNN, a mobile-specific optimization framework that implements GPU offloading specifically for mobile GPUs. Evaluations using an RNN model for activity recognition shows that MobiRNN does significantly decrease the latency of running RNN models on phones.}, booktitle = {Proceedings of the 1st International Workshop on Deep Learning for Mobile Systems and Applications}, pages = {1–6}, numpages = {6}, keywords = {recurrent neural network, renderscript, mobile GPU, performance optimizations}, location = {Niagara Falls, New York, USA}, series = {EMDL '17} }
 
@misc{paine2013gpu,
      title={GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training}, 
      author={Thomas Paine and Hailin Jin and Jianchao Yang and Zhe Lin and Thomas Huang},
      year={2013},
      eprint={1312.6186},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@INPROCEEDINGS{8090194,
  author={Cengil, Emine and Cinar, Ahmet and Gueler, Zafer},
  booktitle={2017 International Artificial Intelligence and Data Processing Symposium (IDAP)}, 
  title={A GPU-based convolutional neural network approach for image classification}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/IDAP.2017.8090194}}

@online{Correct_inference_measure,
  author = { Amnon Geifman },
  title = {The Correct Way to Measure Inference Time of Deep Neural Networks},
  year = 2021,
  url = {https://deci.ai/resources/blog/measure-inference-time-deep-neural-networks/},
  urldate = {2020-04-04}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@online{intel_bench_suite,
  author = { Gennady Fedorov Shaojuan Zhu and Abhinav Singh },
  title = {Intel® oneAPI Math Kernel Library (oneMKL) Benchmarks Suite},
  year = 2021,
  url = {https://www.intel.com/content/www/us/en/developer/articles/technical/onemkl-benchmarks-suite.html},
  urldate = {2021-11-22}
}
@INPROCEEDINGS{118273,
  author={Perugini and Engeler},
  booktitle={International 1989 Joint Conference on Neural Networks}, 
  title={Neural network learning time: effects of network and training set size}, 
  year={1989},
  volume={},
  number={},
  pages={395-401 vol.2},
  doi={10.1109/IJCNN.1989.118273}}

@INPROCEEDINGS{8573476,
  author={Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
  booktitle={2018 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Benchmarking and Analyzing Deep Neural Network Training}, 
  year={2018},
  volume={},
  number={},
  pages={88-100},
  doi={10.1109/IISWC.2018.8573476}}
@misc{han2016eie,
      title={EIE: Efficient Inference Engine on Compressed Deep Neural Network}, 
      author={Song Han and Xingyu Liu and Huizi Mao and Jing Pu and Ardavan Pedram and Mark A. Horowitz and William J. Dally},
      year={2016},
      eprint={1602.01528},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@book{machine_learning,
  title = {Machine Learning},
  author = {Tom M Mitchell},
  year = {1997},
  publisher = {New York  McGraw-Hill},
}
@article{murphy2016overview,
  title={An overview of convolutional neural network architectures for deep learning},
  author={Murphy, John},
  journal={Microway Inc},
  year={2016}
}
@misc{rhu2016vdnn,
      title={vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design}, 
      author={Minsoo Rhu and Natalia Gimelshein and Jason Clemons and Arslan Zulfiqar and Stephen W. Keckler},
      year={2016},
      eprint={1602.08124},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}
@misc{bojarski2016end,
      title={End to End Learning for Self-Driving Cars}, 
      author={Mariusz Bojarski and Davide Del Testa and Daniel Dworakowski and Bernhard Firner and Beat Flepp and Prasoon Goyal and Lawrence D. Jackel and Mathew Monfort and Urs Muller and Jiakai Zhang and Xin Zhang and Jake Zhao and Karol Zieba},
      year={2016},
      eprint={1604.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{huval2015empirical,
      title={An Empirical Evaluation of Deep Learning on Highway Driving}, 
      author={Brody Huval and Tao Wang and Sameep Tandon and Jeff Kiske and Will Song and Joel Pazhayampallil and Mykhaylo Andriluka and Pranav Rajpurkar and Toki Migimatsu and Royce Cheng-Yue and Fernando Mujica and Adam Coates and Andrew Y. Ng},
      year={2015},
      eprint={1504.01716},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
@proceedings{10.1145/2959100,
title = {RecSys '16: Proceedings of the 10th ACM Conference on Recommender Systems},
year = {2016},
isbn = {9781450340359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 10th ACM Conference on Recommender Systems (RecSys 2016), held in Boston, MA, USA, from September 15th through 19th. Since it was first held ten years ago, RecSys has grown to become the leading conference for the presentation and discussion of recommender systems research, bringing together the world's leading recommender systems researchers and e-commerce companies.The program for RecSys 2016 reflects the growth of the Recommender Systems community. For the second time in the history of RecSys we will offer two parallel tracks during the three days of the main conference with a record breaking 112 contributions including 51 technical papers, 9 Past, Present, and Future papers, 15 industry papers, four tutorials, three keynotes, and 30 demos and posters. We again offer an extensive pre-conference program with nine workshops, the RecSys Challenge and a doctoral symposium.The technical program for RecSys 2016 drew upon a record 294 total submissions. To celebrate the tenth year of the conference, the program features a new track reflecting on past, present, and future research in the field of recommender systems. Papers in this track consider a broad perspective on how the field has evolved and the challenges and directions that lie ahead. The review process for all tracks was highly selective. In the main program, 29 long papers were accepted out of 159 submissions (18.2% acceptance rate), 22 out of 110 short papers (20% acceptance rate), and 9 out of 25 Past, Present and Future paper submissions (36% acceptance rate). Prominent topics covered by these papers include human factors, social aspects, context awareness, cold start, novelty and diversity, and core algorithmic research (matrix factorization, deep learning, probabilistic approaches, etc.).Building on the tradition established by previous years, RecSys 2016 features a strong focus on significant real-world challenges facing industrial practitioners and practical solutions to those challenges. The three industry sessions feature a rich set of talks from Mendeley, Meetup, Bloomberg, Foursquare, Spotify, Netflix, Pandora, Stitch Fix, Expedia, Nara Logics, GraphSQL, Retail Rocket, Quora, Google and Pinterest. A wide range of domains are represented in these sessions including publishing, news, Q &amp; A, events, music, movies, television, fashion, apps and games.},
location = {Boston, Massachusetts, USA}
}

@misc{amodei2015deep,
      title={Deep Speech 2: End-to-End Speech Recognition in English and Mandarin}, 
      author={Dario Amodei and Rishita Anubhai and Eric Battenberg and Carl Case and Jared Casper and Bryan Catanzaro and Jingdong Chen and Mike Chrzanowski and Adam Coates and Greg Diamos and Erich Elsen and Jesse Engel and Linxi Fan and Christopher Fougner and Tony Han and Awni Hannun and Billy Jun and Patrick LeGresley and Libby Lin and Sharan Narang and Andrew Ng and Sherjil Ozair and Ryan Prenger and Jonathan Raiman and Sanjeev Satheesh and David Seetapun and Shubho Sengupta and Yi Wang and Zhiqian Wang and Chong Wang and Bo Xiao and Dani Yogatama and Jun Zhan and Zhenyao Zhu},
      year={2015},
      eprint={1512.02595},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Afaq2020SignificanceOE,
  title={Significance Of Epochs On Training A Neural Network},
  author={Saahil Afaq and Smitha Rao},
  journal={International Journal of Scientific \& Technology Research},
  year={2020},
  volume={9},
  pages={485-488}
}

@article{Unterthiner2020PredictingNN,
  title={Predicting Neural Network Accuracy from Weights},
  author={Thomas Unterthiner and Daniel Keysers and Sylvain Gelly and Olivier Bousquet and Ilya O. Tolstikhin},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.11448}
}

@misc{hooker2019benchmark,
      title={A Benchmark for Interpretability Methods in Deep Neural Networks}, 
      author={Sara Hooker and Dumitru Erhan and Pieter-Jan Kindermans and Been Kim},
      year={2019},
      eprint={1806.10758},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{google_doc, 
author = {Google}, 
title = {Machine Learning Crash Course}, 
howpublished = "\url{https://developers.google.com/machine-learning/crash-course/classification/accuracy}", 
year = {2020-02-10},
note = "[Online; accessed 27-12-2021]"
}


@misc{fastaidocs,
  author = {fast.ai},
  title = {{Fastai documentation}},
  howpublished = "\url{https://docs.fast.ai/}",
  year = {Nov 29, 2021}, 
  note = "[Online; accessed 27-12-2021]"
}


@Article{fastai,
AUTHOR = {Howard, Jeremy and Gugger, Sylvain},
TITLE = {Fastai: A Layered API for Deep Learning},
JOURNAL = {Information},
VOLUME = {11},
YEAR = {2020},
NUMBER = {2},
ARTICLE-NUMBER = {108},
URL = {https://www.mdpi.com/2078-2489/11/2/108},
ISSN = {2078-2489},
ABSTRACT = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4&ndash;5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.},
DOI = {10.3390/info11020108}
}


@article{DBLP:journals/corr/abs-1708-07120,
  author    = {Leslie N. Smith and
               Nicholay Topin},
  title     = {Super-Convergence: Very Fast Training of Residual Networks Using Large
               Learning Rates},
  journal   = {CoRR},
  volume    = {abs/1708.07120},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07120},
  eprinttype = {arXiv},
  eprint    = {1708.07120},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-07120.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1803-09820,
  author    = {Leslie N. Smith},
  title     = {A disciplined approach to neural network hyper-parameters: Part 1
               - learning rate, batch size, momentum, and weight decay},
  journal   = {CoRR},
  volume    = {abs/1803.09820},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.09820},
  eprinttype = {arXiv},
  eprint    = {1803.09820},
  timestamp = {Mon, 13 Aug 2018 16:46:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-09820.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{hussein,
title = {Chapter 4 - Energy-efficient EEG monitoring systems for wireless epileptic seizure detection},
editor = {Amr Mohamed},
booktitle = {Energy Efficiency of Medical Devices and Healthcare Applications},
publisher = {Academic Press},
pages = {69-85},
year = {2020},
isbn = {978-0-12-819045-6},
doi = {https://doi.org/10.1016/B978-0-12-819045-6.00004-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128190456000042},
author = {Ramy Hussein and Rabab Ward},
keywords = {EEG signals, Energy-efficient, Expectation maximization, Missing at random, Seizure detection},
abstract = {This chapter addresses one of the major challenges faced by wireless seizure detection systems—the limited battery lifetime of electroencephalogram (EEG) sensor units. A wireless EEG device enables ambulatory EEG monitoring outside clinical settings, that is, while patients are freely moving around performing their daily activities. Such a device, however, requires a considerable amount of energy to acquire and encode the EEG data and to transmit it to the server side (where seizure detection is carried out). Presently, there exist three main EEG monitoring paradigms. These are based on (i) transmitting the entire raw EEG data, (ii) transmitting the compressed EEG data, or (iii) transmitting certain EEG features. In this chapter, we show how to modify each of the previously mentioned paradigms so that the total power consumption at the sensor side is reduced while maintaining high seizure detection accuracy at the server side. First, a new module named “missing at random (MAR)” is added to the sensor unit. This module deletes some EEG data points randomly, hence reducing the amount of data that has to be transmitted, and certainly reducing the energy required for data transmission at the sensor side. Second, we amend the data server to have a new module named “expectation maximization (EM),” which is used to accurately estimate the missing (intentionally deleted) EEG data points. Experimental results show that adding the MAR and EM modules to existing EEG monitoring paradigms results in ∼60% saving in power consumption. The battery lifetime of these paradigms is doubled, while a superior seizure detection accuracy of 95%–99% is achieved.}
}
@misc{hendrycks2019benchmarking,
      title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations}, 
      author={Dan Hendrycks and Thomas Dietterich},
      year={2019},
      eprint={1903.12261},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{turbo_boost,
author = {Acun, Bilge and Miller, Phil and Kalé, Laxmikant},
year = {2016},
month = {06},
pages = {1-12},
title = {Variation Among Processors Under Turbo Boost in HPC Systems},
doi = {10.1145/2925426.2926289}
}
@misc{tatbul2019precision,
      title={Precision and Recall for Time Series}, 
      author={Nesime Tatbul and Tae Jun Lee and Stan Zdonik and Mejbah Alam and Justin Gottschlich},
      year={2019},
      eprint={1803.03639},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{10.5555/2559492,
author = {He, Haibo and Ma, Yunqian},
title = {Imbalanced Learning: Foundations, Algorithms, and Applications},
year = {2013},
isbn = {1118074629},
publisher = {Wiley-IEEE Press},
edition = {1st},
abstract = {The first book of its kind to review the current status and future direction of the exciting new branch of machine learning/data mining called imbalanced learningImbalanced learning focuses on how an intelligent system can learn when it is provided with imbalanced data. Solving imbalanced learning problems is critical in numerous data-intensive networked systems, including surveillance, security, Internet, finance, biomedical, defense, and more. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. The first comprehensive look at this new branch of machine learning, this book offers a critical review of the problem of imbalanced learning, covering the state of the art in techniques, principles, and real-world applications. Featuring contributions from experts in both academia and industry, Imbalanced Learning: Foundations, Algorithms, and Applications provides chapter coverage on:Foundations of Imbalanced LearningImbalanced Datasets: From Sampling to ClassifiersEnsemble Methods for Class Imbalance LearningClass Imbalance Learning Methods for Support Vector MachinesClass Imbalance and Active LearningNonstationary Stream Data Learning with Imbalanced Class DistributionAssessment Metrics for Imbalanced LearningImbalanced Learning: Foundations, Algorithms, and Applications will help scientists and engineers learn how to tackle the problem of learning from imbalanced datasets, and gain insight into current developments in the field as well as future research directions.}
}
@inproceedings{metrics,
author = {Sokolova, Marina and Japkowicz, Nathalie and Szpakowicz, Stan},
year = {2006},
month = {01},
pages = {1015-1021},
title = {Beyond Accuracy, F-Score and ROC: A Family of Discriminant Measures for Performance Evaluation},
volume = {Vol. 4304},
isbn = {978-3-540-49787-5},
journal = {AI 2006: Advances in Artificial Intelligence, Lecture Notes in Computer Science},
doi = {10.1007/11941439_114}
}

@article{Rijsbergen1974FOUNDATIONOE,
  title={FOUNDATION OF EVALUATION},
  author={C. J. van Rijsbergen},
  journal={Journal of Documentation},
  year={1974},
  volume={30},
  pages={365-373}
}

@inproceedings{derczynski-2016-complementarity,
    title = "Complementarity, {F}-score, and {NLP} Evaluation",
    author = "Derczynski, Leon",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1040",
    pages = "261--266",
    abstract = "This paper addresses the problem of quantifying the differences between entity extraction systems, where in general only a small proportion a document should be selected. Comparing overall accuracy is not very useful in these cases, as small differences in accuracy may correspond to huge differences in selections over the target minority class. Conventionally, one may use per-token complementarity to describe these differences, but it is not very useful when the set is heavily skewed. In such situations, which are common in information retrieval and entity recognition, metrics like precision and recall are typically used to describe performance. However, precision and recall fail to describe the differences between sets of objects selected by different decision strategies, instead just describing the proportional amount of correct and incorrect objects selected. This paper presents a method for measuring complementarity for precision, recall and F-score, quantifying the difference between entity extraction approaches.",
}
@article{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14,
  author    = {Olga Russakovsky and
               Jia Deng and
               Hao Su and
               Jonathan Krause and
               Sanjeev Satheesh and
               Sean Ma and
               Zhiheng Huang and
               Andrej Karpathy and
               Aditya Khosla and
               Michael S. Bernstein and
               Alexander C. Berg and
               Li Fei{-}Fei},
  title     = {ImageNet Large Scale Visual Recognition Challenge},
  journal   = {CoRR},
  volume    = {abs/1409.0575},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.0575},
  eprinttype = {arXiv},
  eprint    = {1409.0575},
  timestamp = {Wed, 15 Sep 2021 14:13:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RussakovskyDSKSMHKKBBF14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/HeZRS15,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  eprinttype = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{simonyan2015deep,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}
@InProceedings{parkhi12a,
  author       = "Omkar M. Parkhi and Andrea Vedaldi and Andrew Zisserman and C. V. Jawahar",
  title        = "Cats and Dogs",
  booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition",
  year         = "2012",
}
@article{suh_transfer_2018,
	title = {Transfer learning for the classification of sugar beet and volunteer potato under field conditions},
	volume = {174},
	issn = {15375110},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1537511016308777},
	doi = {10.1016/j.biosystemseng.2018.06.017},
	language = {en},
	urldate = {2021-09-08},
	journal = {Biosystems Engineering},
	author = {Suh, Hyun K. and IJsselmuiden, Joris and Hofstee, Jan Willem and van Henten, Eldert J.},
	month = oct,
	year = {2018},
	pages = {50--65},
}
@book{10.5555/525960, author = {Bishop, Christopher M.}, title = {Neural Networks for Pattern Recognition}, year = {1995}, isbn = {0198538642}, publisher = {Oxford University Press, Inc.}, address = {USA}, abstract = {From the Publisher:This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.},
pages = {539}, }

@book{reed_neural_1999,
	title = {Neural smithing supervised learning in feedforward artificial neural networks},
	isbn = {978-0-585-07838-0 978-0-262-28221-5 978-0-262-52701-9},
	url = {https://doi.org/10.7551/mitpress/4937.001.0001},
	abstract = {"Artificial neural networks are nonlinear mapping systems whose structure is loosely based on principles observed in the nervous systems of humans and animals. The basic idea is that massive systems of simple units linked together in appropriate ways can generate many complex and interesting behaviors. This book focuses on the subset of feedforward artificial neural networks called multilayer perceptions (MLP). These are the most widely used neural networks, with applications as diverse as finance (forecasting), manufacturing (process control), and science (speech and image recognition)." "This book presents an extensive and practical overview of almost every aspect of MLP methodology, progressing from an initial discussion of what MLPs are and how they might be used to an in-depth examination of technical factors affecting performance. The book can be used as a tool kit by readers interested in applying networks to specific problems, yet it also presents theory and references outlining the last ten years of MLP research."--Jacket.},
	language = {English},
	urldate = {2021-12-31},
	author = {Reed, Russell D and Marks, Robert J},
	year = {1999},
	note = {OCLC: 1227498094},
}
@article{Nowlan1992SimplifyingNN,
  title={Simplifying Neural Networks by Soft Weight-Sharing},
  author={Steven J. Nowlan and Geoffrey E. Hinton},
  journal={Neural Computation},
  year={1992},
  volume={4},
  pages={473-493}
}
@inproceedings{overfitting,
author = {Lawrence, Steve and Giles, C. and Tsoi, Ah},
year = {1997},
month = {01},
pages = {540-545},
title = {Lessons in Neural Network Training: Overfitting May be Harder than Expected.},
journal = {Proceedings of the National Conference on Artificial Intelligence}
}
@article{cioni_weed_2010,
	title = {Weed {Control} in {Sugarbeet}},
	volume = {12},
	issn = {0972-1525, 0974-0740},
	url = {http://link.springer.com/10.1007/s12355-010-0036-2},
	doi = {10.1007/s12355-010-0036-2},
	language = {en},
	number = {3-4},
	urldate = {2021-08-20},
	journal = {Sugar Tech},
	author = {Cioni, Franco and Maines, Gianfranco},
	month = dec,
	year = {2010},
	pages = {243--255},
}
@article{petersen_review_2004,
	title = {A {Review} on {Weed} {Control} in {Sug}- arbeet.},
	journal = {Inderjit (Ed), Weed Biology and Man- agement.},
	author = {Petersen, J.},
	year = {2004},
}
@article{raja_real-time_2020,
	title = {Real-time robotic weed knife control system for tomato and lettuce based on geometric appearance of plant labels},
	volume = {194},
	issn = {15375110},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1537511020300854},
	doi = {10.1016/j.biosystemseng.2020.03.022},
	language = {en},
	urldate = {2021-08-31},
	journal = {Biosystems Engineering},
	author = {Raja, Rekha and Nguyen, Thuy T. and Slaughter, David C. and Fennimore, Steven A.},
	month = jun,
	year = {2020},
	pages = {152--164},
}
@article{frasconi_design_2014,
	title = {Design and full realization of physical weed control ({PWC}) automated machine within the {RHEA} project},
	author = {Frasconi, C. and Martelloni, L. and Fontanelli, M. and Raffaelli, M. and Emmi, L. and Pirchio, Michel and Peruzzi, A.},
	year = {2014},
}
@article{machleb_sensor-based_2021,
	title = {Sensor-{Based} {Intrarow} {Mechanical} {Weed} {Control} in {Sugar} {Beets} with {Motorized} {Finger} {Weeders}},
	volume = {11},
	issn = {2073-4395},
	url = {https://www.mdpi.com/2073-4395/11/8/1517},
	doi = {10.3390/agronomy11081517},
	abstract = {The need for herbicide usage reduction and the increased interest in mechanical weed control has prompted greater attention to the development of agricultural robots for autonomous weeding in the past years. This also requires the development of suitable mechanical weeding tools. Therefore, we devised a new weeding tool for agricultural robots to perform intrarow mechanical weed control in sugar beets. A conventional finger weeder was modified and equipped with an electric motor. This allowed the rotational movement of the finger weeders independent of the forward travel speed of the tool carrier. The new tool was tested in combination with a bi-spectral camera in a two-year field trial. The camera was used to identify crop plants in the intrarow area. A controller regulated the speed of the motorized finger weeders, realizing two different setups. At the location of a sugar beet plant, the rotational speed was equal to the driving speed of the tractor. Between two sugar beet plants, the rotational speed was either increased by 40\% or decreased by 40\%. The intrarow weed control efficacy of this new system ranged from 87 to 91\% in 2017 and from 91 to 94\% in 2018. The sugar beet yields were not adversely affected by the mechanical treatments compared to the conventional herbicide application. The motorized finger weeders present an effective system for selective intrarow mechanical weeding. Certainly, mechanical weeding involves the risk of high weed infestations if the treatments are not applied properly and in a timely manner regardless of whether sensor technology is used or not. However, due to the increasing herbicide resistances and the continuing bans on herbicides, mechanical weeding strategies must be investigated further. The mechanical weeding system of the present study can contribute to the reduction of herbicide use in sugar beets and other wide row crops.},
	language = {en},
	number = {8},
	urldate = {2021-09-09},
	journal = {Agronomy},
	author = {Machleb, Jannis and Peteinatos, Gerassimos G. and Sökefeld, Markus and Gerhards, Roland},
	month = jul,
	year = {2021},
	pages = {1517},
	file = {Full Text:/Users/brodie/Zotero/storage/A8Q3LMMJ/Machleb et al. - 2021 - Sensor-Based Intrarow Mechanical Weed Control in S.pdf:application/pdf},
}

@article{hasan_survey_2021,
	title = {A {Survey} of {Deep} {Learning} {Techniques} for {Weed} {Detection} from {Images}},
	url = {http://arxiv.org/abs/2103.01415},
	abstract = {The rapid advances in Deep Learning (DL) techniques have enabled rapid detection, localisation, and recognition of objects from images or videos. DL techniques are now being used in many applications related to agriculture and farming. Automatic detection and classification of weeds can play an important role in weed management and so contribute to higher yields. Weed detection in crops from imagery is inherently a challenging problem because both weeds and crops have similar colours ('green-on-green'), and their shapes and texture can be very similar at the growth phase. Also, a crop in one setting can be considered a weed in another. In addition to their detection, the recognition of specific weed species is essential so that targeted controlling mechanisms (e.g. appropriate herbicides and correct doses) can be applied. In this paper, we review existing deep learning-based weed detection and classification techniques. We cover the detailed literature on four main procedures, i.e., data acquisition, dataset preparation, DL techniques employed for detection, location and classification of weeds in crops, and evaluation metrics approaches. We found that most studies applied supervised learning techniques, they achieved high classification accuracy by fine-tuning pre-trained models on any plant dataset, and past experiments have already achieved high accuracy when a large amount of labelled data is available.},
	urldate = {2021-08-16},
	journal = {arXiv:2103.01415 [cs]},
	author = {Hasan, A. S. M. Mahmudul and Sohel, Ferdous and Diepeveen, Dean and Laga, Hamid and Jones, Michael G. K.},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.01415},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/brodie/Zotero/storage/DU2YCQA5/Hasan et al. - 2021 - A Survey of Deep Learning Techniques for Weed Dete.pdf:application/pdf;arXiv.org Snapshot:/Users/brodie/Zotero/storage/BTATPR2J/2103.html:text/html},
}
@article{gao_deep_2020,
	title = {Deep convolutional neural networks for image-based {Convolvulus} sepium detection in sugar beet fields},
	volume = {16},
	issn = {1746-4811},
	url = {https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-00570-z},
	doi = {10.1186/s13007-020-00570-z},
	abstract = {Abstract
            
              Background
              
                Convolvulus sepium
                (hedge bindweed) detection in sugar beet fields remains a challenging problem due to variation in appearance of plants, illumination changes, foliage occlusions, and different growth stages under field conditions. Current approaches for weed and crop recognition, segmentation and detection rely predominantly on conventional machine-learning techniques that require a large set of hand-crafted features for modelling. These might fail to generalize over different fields and environments.
              
            
            
              Results
              
                Here, we present an approach that develops a deep convolutional neural network (CNN) based on the tiny YOLOv3 architecture for
                C. sepium
                and sugar beet detection. We generated 2271 synthetic images, before combining these images with 452 field images to train the developed model. YOLO anchor box sizes were calculated from the training dataset using a k-means clustering approach. The resulting model was tested on 100 field images, showing that the combination of synthetic and original field images to train the developed model could improve the mean average precision (mAP) metric from 0.751 to 0.829 compared to using collected field images alone. We also compared the performance of the developed model with the YOLOv3 and Tiny YOLO models. The developed model achieved a better trade-off between accuracy and speed. Specifically, the average precisions (APs@IoU0.5) of
                C. sepium
                and sugar beet were 0.761 and 0.897 respectively with 6.48 ms inference time per image (800 × 1200) on a NVIDIA Titan X GPU environment.
              
            
            
              Conclusion
              The developed model has the potential to be deployed on an embedded mobile platform like the Jetson TX for online weed detection and management due to its high-speed inference. It is recommendable to use synthetic images and empirical field images together in training stage to improve the performance of models.},
	language = {en},
	number = {1},
	urldate = {2021-08-23},
	journal = {Plant Methods},
	author = {Gao, Junfeng and French, Andrew P. and Pound, Michael P. and He, Yong and Pridmore, Tony P. and Pieters, Jan G.},
	month = dec,
	year = {2020},
	pages = {29},
	file = {Full Text:/Users/brodie/Zotero/storage/HIIW95F9/Gao et al. - 2020 - Deep convolutional neural networks for image-based.pdf:application/pdf},
}

@article{ramirez_deep_2020,
	title = {{DEEP} {CONVOLUTIONAL} {NEURAL} {NETWORKS} {FOR} {WEED} {DETECTION} {IN} {AGRICULTURAL} {CROPS} {USING} {OPTICAL} {AERIAL} {IMAGES}},
	volume = {XLII-3/W12-2020},
	issn = {2194-9034},
	url = {https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLII-3-W12-2020/551/2020/},
	doi = {10.5194/isprs-archives-XLII-3-W12-2020-551-2020},
	abstract = {Abstract. The presence of weeds in agricultural crops has been one of the problems of greatest interest in recent years as they consume natural resources and negatively affect the agricultural process. For this purpose, a model has been implemented to segment weed in aerial images. The proposed model relies on DeepLabv3 architecture trained upon patches extracted from high-resolution aerial imagery. The dataset employed consisted in 5 high-resolution images that describes a sugar beet agricultural field in Germany. SegNet and U-Net architectures were selected for comparison purposes. Our results demonstrate that balancing of data, together with a greater spatial context leads better results with DeepLabv3 achieving up to 0.89 and 0.81 in terms of AUC and F1-score, respectively.},
	language = {en},
	urldate = {2021-09-08},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Ramirez, W. and Achanccaray, P. and Mendoza, L. F. and Pacheco, M. A. C.},
	month = nov,
	year = {2020},
	pages = {551--555},
	file = {Full Text:/Users/brodie/Zotero/storage/8DLZ7P4H/Ramirez et al. - 2020 - DEEP CONVOLUTIONAL NEURAL NETWORKS FOR WEED DETECT.pdf:application/pdf},
}


@INPROCEEDINGS{7487720,
  author={Lottes, P. and Hoeferlin, M. and Sander, S. and Müter, M. and Schulze, P. and Stachniss, Lammers C.},
  booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={An effective classification system for separating sugar beets and weeds for precision farming applications}, 
  year={2016},
  volume={},
  number={},
  pages={5157-5163},
  doi={10.1109/ICRA.2016.7487720}}


@misc{giselsson2017public,
      title={A Public Image Database for Benchmark of Plant Seedling Classification Algorithms}, 
      author={Thomas Mosgaard Giselsson and Rasmus Nyholm Jørgensen and Peter Kryger Jensen and Mads Dyrmann and Henrik Skov Midtiby},
      year={2017},
      eprint={1711.05458},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}



