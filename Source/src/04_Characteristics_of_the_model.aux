\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\citation{gawlikowski2021survey}
\citation{Number_of_DL_papers}
\citation{Number_of_DL_papers}
\citation{gawlikowski2021survey}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Characteristics of Neural Networks}{17}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\newlabel{char_nn}{{2}{17}{Characteristics of Neural Networks}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Uncertainty in Deep Neural Networks}{17}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Trend in publications about Deep Learning}}{17}{figure.caption.7}\protected@file@percent }
\newlabel{fig:annual_trend}{{2.1}{17}{Trend in publications about Deep Learning}{figure.caption.7}{}}
\citation{uncertainity_classi}
\citation{ovadia2019trust}
\citation{gawlikowski2021survey}
\citation{gawlikowski2021survey}
\citation{gawlikowski2021survey}
\citation{gawlikowski2021survey}
\citation{ruder2017overview}
\citation{ruder2017overview}
\citation{nguyen2015deep}
\citation{nguyen2015deep}
\citation{nguyen2015deep}
\citation{nguyen2015deep}
\newlabel{eq:DNN_model}{{2.1}{18}{Uncertainty in Deep Neural Networks}{equation.2.1.1}{}}
\citation{Separation_uncer}
\citation{DBLP:journals/corr/abs-1811-01412}
\citation{DBLP:journals/corr/abs-1811-01412}
\citation{Separation_uncer}
\citation{gawlikowski2021survey}
\citation{DBLP:journals/corr/abs-1811-01412}
\citation{DBLP:journals/corr/KendallG17}
\citation{Separation_uncer}
\citation{Separation_uncer}
\citation{DBLP:journals/corr/abs-1811-01412}
\citation{DBLP:journals/corr/KendallG17}
\citation{KIUREGHIAN2009105}
\citation{Separation_uncer}
\citation{gawlikowski2021survey}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Differences between how DNNs and humans recognize objects}}{19}{figure.caption.8}\protected@file@percent }
\newlabel{fig:fooling_DNN}{{2.2}{19}{Differences between how DNNs and humans recognize objects}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Illustrating the difference between aleatoric and epistemic uncertainty}}{20}{figure.caption.9}\protected@file@percent }
\newlabel{fig:un_visua}{{2.3}{20}{Illustrating the difference between aleatoric and epistemic uncertainty}{figure.caption.9}{}}
\citation{machine_learning}
\citation{Goodfellow-et-al-2016}
\citation{murphy2016overview}
\citation{murphy2016overview}
\citation{murphy2016overview}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Learning Process and Training Time}{21}{section.2.2}\protected@file@percent }
\newlabel{sec:training_time}{{2.2}{21}{Learning Process and Training Time}{section.2.2}{}}
\citation{murphy2016overview}
\citation{8573476}
\citation{rhu2016vdnn}
\citation{8573476}
\citation{bojarski2016end}
\citation{huval2015empirical}
\citation{10.1145/2959100}
\citation{amodei2015deep}
\citation{8573476}
\citation{8573476}
\citation{han2016eie}
\citation{8573476}
\citation{hendrycks2019benchmarking}
\citation{bianco2018dnnsbench}
\citation{unterthiner2021predicting}
\citation{hussein}
\newlabel{eq:MSE}{{2.3}{22}{Learning Process and Training Time}{equation.2.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Inference Time}{22}{section.2.3}\protected@file@percent }
\newlabel{sec:inference_time_definition}{{2.3}{22}{Inference Time}{section.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Accuracy}{22}{section.2.4}\protected@file@percent }
\newlabel{sec:accuracy}{{2.4}{22}{Accuracy}{section.2.4}{}}
\citation{google_doc}
\citation{metrics}
\citation{tatbul2019precision}
\newlabel{eq:cla_acc}{{2.4}{23}{Accuracy}{equation.2.4.4}{}}
\newlabel{eq:bin_acc}{{2.5}{23}{Accuracy}{equation.2.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Example for binary classification\relax }}{23}{table.caption.10}\protected@file@percent }
\newlabel{tab:tumor}{{2.1}{23}{Example for binary classification\relax }{table.caption.10}{}}
\newlabel{eq:bin_acc2}{{2.6}{23}{Accuracy}{equation.2.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Precision and Recall}{23}{section.2.5}\protected@file@percent }
\citation{tatbul2019precision}
\citation{10.5555/2559492}
\citation{Rijsbergen1974FOUNDATIONOE}
\citation{derczynski-2016-complementarity}
\newlabel{eq:pre}{{2.7}{24}{Precision and Recall}{equation.2.5.7}{}}
\newlabel{eq:rec}{{2.8}{24}{Precision and Recall}{equation.2.5.8}{}}
\newlabel{eq:pre_ex}{{2.9}{24}{Precision and Recall}{equation.2.5.9}{}}
\newlabel{eq:rec2}{{2.10}{24}{Precision and Recall}{equation.2.5.10}{}}
\newlabel{eq:f_score}{{2.11}{24}{Precision and Recall}{equation.2.5.11}{}}
\citation{derczynski-2016-complementarity}
\citation{Goodfellow-et-al-2016}
\citation{reed_neural_1999}
\citation{Goodfellow-et-al-2016}
\citation{dietterich1995overfitting}
\citation{jabbar2015methods}
\citation{dietterich1995overfitting}
\citation{10.1016/j.inffus.2008.11.003}
\citation{10.1016/j.inffus.2008.11.003}
\newlabel{eq:f1_score}{{2.12}{25}{Precision and Recall}{equation.2.5.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Loss}{25}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Overfitting and Underfitting}{25}{section.2.7}\protected@file@percent }
\newlabel{sec:of_uf}{{2.7}{25}{Overfitting and Underfitting}{section.2.7}{}}
\citation{FINNOFF1993771}
\citation{early_stopping}
\citation{early_stopping}
\citation{early_stopping}
\citation{early_stopping}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Validation and training loss curve}}{26}{figure.caption.11}\protected@file@percent }
\newlabel{fig:over_fitting_curve}{{2.4}{26}{Validation and training loss curve}{figure.caption.11}{}}
\citation{early_stopping}
\citation{early_stopping}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Real Example of the Validation and training loss curve}}{27}{figure.caption.12}\protected@file@percent }
\newlabel{fig:over_fitting_curve_real}{{2.5}{27}{Real Example of the Validation and training loss curve}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Real Example of the Validation and training loss curve}}{27}{figure.caption.13}\protected@file@percent }
\newlabel{fig:over_fitting_curve_2}{{2.6}{27}{Real Example of the Validation and training loss curve}{figure.caption.13}{}}
\citation{suh_transfer_2018}
\citation{s20205893}
\citation{phdthesis}
\citation{NIPS2012_c399862d}
\citation{NIPS2012_c399862d}
\citation{NIPS2012_c399862d}
\citation{NIPS2012_c399862d}
\citation{NIPS2012_c399862d}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Architecture}{28}{section.2.8}\protected@file@percent }
\newlabel{sec:arch}{{2.8}{28}{Architecture}{section.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.1}Alexnet}{28}{subsection.2.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Overview of the architecture of Alexnet}}{28}{figure.caption.14}\protected@file@percent }
\newlabel{fig:alexnet_architecture}{{2.7}{28}{Overview of the architecture of Alexnet}{figure.caption.14}{}}
\citation{simonyan2015deep}
\citation{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}
\citation{simonyan2015deep}
\citation{simonyan2015deep}
\citation{simonyan2015deep}
\citation{simonyan2015deep}
\citation{szegedy2014going}
\citation{simonyan2015deep}
\citation{ioffe2015batch}
\citation{girshick2014rich}
\citation{2014Spatial}
\citation{DBLP:journals/corr/HeZRS15}
\citation{DBLP:journals/corr/HeZRS15}
\citation{DBLP:journals/corr/HeZRS15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.2}VGG Networks}{29}{subsection.2.8.2}\protected@file@percent }
\newlabel{sec:VGG}{{2.8.2}{29}{VGG Networks}{subsection.2.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.3}Residual Neural Networks}{29}{subsection.2.8.3}\protected@file@percent }
\citation{DBLP:journals/corr/HeZRS15}
\citation{DBLP:journals/corr/HeZRS15}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Overview of the various architecture for VGG}}{30}{figure.caption.15}\protected@file@percent }
\newlabel{fig:vgg_arch}{{2.8}{30}{Overview of the various architecture for VGG}{figure.caption.15}{}}
\newlabel{eq:residual_blocks}{{2.13}{30}{Residual Neural Networks}{equation.2.8.13}{}}
\newlabel{eq:residual_function}{{2.14}{30}{Residual Neural Networks}{equation.2.8.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Residual learning: a building block}}{31}{figure.caption.16}\protected@file@percent }
\newlabel{fig:res_block}{{2.9}{31}{Residual learning: a building block}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Overview of the Resnet Architecture}}{31}{figure.caption.17}\protected@file@percent }
\newlabel{fig:resnet_arch}{{2.10}{31}{Overview of the Resnet Architecture}{figure.caption.17}{}}
\@setckpt{src/04_Characteristics_of_the_model}{
\setcounter{page}{32}
\setcounter{equation}{14}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{8}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{1}
\setcounter{Item}{9}
\setcounter{Hfootnote}{2}
\setcounter{bookmark@seq@number}{0}
\setcounter{AM@survey}{0}
\setcounter{lstnumber}{1}
\setcounter{svg@param@lastpage}{0}
\setcounter{svg@param@currpage}{-1}
\setcounter{caption@flags}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{31}
\setcounter{FV@TrueTabGroupLevel}{0}
\setcounter{FV@TrueTabCounter}{0}
\setcounter{FV@HighlightLinesStart}{0}
\setcounter{FV@HighlightLinesStop}{0}
\setcounter{FancyVerbLineBreakLast}{0}
\setcounter{float@type}{16}
\setcounter{minted@FancyVerbLineTemp}{0}
\setcounter{minted@pygmentizecounter}{0}
\setcounter{listing}{0}
\setcounter{parentequation}{0}
\setcounter{NAT@ctr}{0}
\setcounter{section@level}{2}
\setcounter{lstlisting}{0}
}
