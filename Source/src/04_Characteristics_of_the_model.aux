\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\citation{gawlikowski2021survey}
\citation{Number_of_DL_papers}
\citation{Number_of_DL_papers}
\citation{gawlikowski2021survey}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Characteristics of Neural Networks}{19}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\newlabel{char_nn}{{3}{19}{Characteristics of Neural Networks}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Deep Neural Networks and Uncertainty}{19}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Trend in publications about Deep Learning}}{19}{figure.caption.7}\protected@file@percent }
\newlabel{fig:annual_trend}{{3.1}{19}{Trend in publications about Deep Learning}{figure.caption.7}{}}
\citation{uncertainity_classi}
\citation{ovadia2019trust}
\citation{gawlikowski2021survey}
\citation{gawlikowski2021survey}
\citation{gawlikowski2021survey}
\citation{gawlikowski2021survey}
\citation{ruder2017overview}
\citation{ruder2017overview}
\citation{nguyen2015deep}
\citation{nguyen2015deep}
\citation{nguyen2015deep}
\citation{nguyen2015deep}
\newlabel{eq:DNN_model}{{3.1}{20}{Deep Neural Networks and Uncertainty}{equation.3.1.1}{}}
\citation{Separation_uncer}
\citation{DBLP:journals/corr/abs-1811-01412}
\citation{DBLP:journals/corr/abs-1811-01412}
\citation{Separation_uncer}
\citation{gawlikowski2021survey}
\citation{DBLP:journals/corr/abs-1811-01412}
\citation{DBLP:journals/corr/KendallG17}
\citation{Separation_uncer}
\citation{Separation_uncer}
\citation{DBLP:journals/corr/abs-1811-01412}
\citation{DBLP:journals/corr/KendallG17}
\citation{KIUREGHIAN2009105}
\citation{Separation_uncer}
\citation{gawlikowski2021survey}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Differences between how DNNs and humans recognize objects}}{21}{figure.caption.8}\protected@file@percent }
\newlabel{fig:fooling_DNN}{{3.2}{21}{Differences between how DNNs and humans recognize objects}{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Illustrating the difference between aleatoric and epistemic uncertainty}}{22}{figure.caption.9}\protected@file@percent }
\newlabel{fig:un_visua}{{3.3}{22}{Illustrating the difference between aleatoric and epistemic uncertainty}{figure.caption.9}{}}
\citation{machine_learning}
\citation{Goodfellow-et-al-2016}
\citation{murphy2016overview}
\citation{murphy2016overview}
\citation{murphy2016overview}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Learning Process and Training Time}{23}{section.3.2}\protected@file@percent }
\newlabel{sec:training_time}{{3.2}{23}{Learning Process and Training Time}{section.3.2}{}}
\citation{murphy2016overview}
\citation{8573476}
\citation{rhu2016vdnn}
\citation{8573476}
\citation{bojarski2016end}
\citation{huval2015empirical}
\citation{10.1145/2959100}
\citation{amodei2015deep}
\citation{8573476}
\citation{8573476}
\citation{han2016eie}
\citation{8573476}
\citation{hendrycks2019benchmarking}
\citation{bianco2018dnnsbench}
\citation{unterthiner2021predicting}
\citation{hussein}
\newlabel{eq:MSE}{{3.3}{24}{Learning Process and Training Time}{equation.3.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Inference Time}{24}{section.3.3}\protected@file@percent }
\newlabel{sec:inference_time_definition}{{3.3}{24}{Inference Time}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Accuracy}{24}{section.3.4}\protected@file@percent }
\newlabel{sec:accuracy}{{3.4}{24}{Accuracy}{section.3.4}{}}
\citation{google_doc}
\citation{metrics}
\citation{tatbul2019precision}
\newlabel{eq:cla_acc}{{3.4}{25}{Accuracy}{equation.3.4.4}{}}
\newlabel{eq:bin_acc}{{3.5}{25}{Accuracy}{equation.3.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Example for binary classification\relax }}{25}{table.caption.10}\protected@file@percent }
\newlabel{tab:tumor}{{3.1}{25}{Example for binary classification\relax }{table.caption.10}{}}
\newlabel{eq:bin_acc2}{{3.6}{25}{Accuracy}{equation.3.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Precision and Recall}{25}{section.3.5}\protected@file@percent }
\citation{tatbul2019precision}
\citation{10.5555/2559492}
\citation{Rijsbergen1974FOUNDATIONOE}
\citation{derczynski-2016-complementarity}
\newlabel{eq:pre}{{3.7}{26}{Precision and Recall}{equation.3.5.7}{}}
\newlabel{eq:rec}{{3.8}{26}{Precision and Recall}{equation.3.5.8}{}}
\newlabel{eq:pre_ex}{{3.9}{26}{Precision and Recall}{equation.3.5.9}{}}
\newlabel{eq:rec2}{{3.10}{26}{Precision and Recall}{equation.3.5.10}{}}
\newlabel{eq:f_score}{{3.11}{26}{Precision and Recall}{equation.3.5.11}{}}
\citation{derczynski-2016-complementarity}
\citation{Goodfellow-et-al-2016}
\citation{reed_neural_1999}
\citation{Goodfellow-et-al-2016}
\citation{dietterich1995overfitting}
\citation{jabbar2015methods}
\citation{dietterich1995overfitting}
\citation{10.1016/j.inffus.2008.11.003}
\citation{10.1016/j.inffus.2008.11.003}
\newlabel{eq:f1_score}{{3.12}{27}{Precision and Recall}{equation.3.5.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Loss}{27}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Overfitting and Underfitting}{27}{section.3.7}\protected@file@percent }
\newlabel{sec:of_uf}{{3.7}{27}{Overfitting and Underfitting}{section.3.7}{}}
\citation{FINNOFF1993771}
\citation{early_stopping}
\citation{early_stopping}
\citation{early_stopping}
\citation{early_stopping}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Validation and training loss curve}}{28}{figure.caption.11}\protected@file@percent }
\newlabel{fig:over_fitting_curve}{{3.4}{28}{Validation and training loss curve}{figure.caption.11}{}}
\citation{early_stopping}
\citation{early_stopping}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Real Example of the Validation and training loss curve}}{29}{figure.caption.12}\protected@file@percent }
\newlabel{fig:over_fitting_curve_real}{{3.5}{29}{Real Example of the Validation and training loss curve}{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Real Example of the Validation and training loss curve}}{29}{figure.caption.13}\protected@file@percent }
\newlabel{fig:over_fitting_curve_2}{{3.6}{29}{Real Example of the Validation and training loss curve}{figure.caption.13}{}}
\citation{suh_transfer_2018}
\citation{s20205893}
\citation{phdthesis}
\citation{NIPS2012_c399862d}
\citation{NIPS2012_c399862d}
\citation{NIPS2012_c399862d}
\citation{NIPS2012_c399862d}
\citation{NIPS2012_c399862d}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Architecture}{30}{section.3.8}\protected@file@percent }
\newlabel{sec:arch}{{3.8}{30}{Architecture}{section.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Alexnet}{30}{subsection.3.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Overview of the architecture of Alexnet}}{30}{figure.caption.14}\protected@file@percent }
\newlabel{fig:alexnet_architecture}{{3.7}{30}{Overview of the architecture of Alexnet}{figure.caption.14}{}}
\citation{simonyan2015deep}
\citation{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14}
\citation{simonyan2015deep}
\citation{simonyan2015deep}
\citation{simonyan2015deep}
\citation{simonyan2015deep}
\citation{szegedy2014going}
\citation{simonyan2015deep}
\citation{ioffe2015batch}
\citation{girshick2014rich}
\citation{2014Spatial}
\citation{DBLP:journals/corr/HeZRS15}
\citation{DBLP:journals/corr/HeZRS15}
\citation{DBLP:journals/corr/HeZRS15}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}VGG Networks}{31}{subsection.3.8.2}\protected@file@percent }
\newlabel{sec:VGG}{{3.8.2}{31}{VGG Networks}{subsection.3.8.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.3}Residual Neural Networks}{31}{subsection.3.8.3}\protected@file@percent }
\citation{DBLP:journals/corr/HeZRS15}
\citation{DBLP:journals/corr/HeZRS15}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Overview of the various architecture for VGG}}{32}{figure.caption.15}\protected@file@percent }
\newlabel{fig:vgg_arch}{{3.8}{32}{Overview of the various architecture for VGG}{figure.caption.15}{}}
\newlabel{eq:residual_blocks}{{3.13}{32}{Residual Neural Networks}{equation.3.8.13}{}}
\newlabel{eq:residual_function}{{3.14}{32}{Residual Neural Networks}{equation.3.8.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Residual learning: a building block}}{33}{figure.caption.16}\protected@file@percent }
\newlabel{fig:res_block}{{3.9}{33}{Residual learning: a building block}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Overview of the Resnet Architecture}}{33}{figure.caption.17}\protected@file@percent }
\newlabel{fig:resnet_arch}{{3.10}{33}{Overview of the Resnet Architecture}{figure.caption.17}{}}
\@setckpt{src/04_Characteristics_of_the_model}{
\setcounter{page}{34}
\setcounter{equation}{14}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{2}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{8}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{1}
\setcounter{Item}{9}
\setcounter{Hfootnote}{2}
\setcounter{bookmark@seq@number}{0}
\setcounter{AM@survey}{0}
\setcounter{lstnumber}{1}
\setcounter{svg@param@lastpage}{0}
\setcounter{svg@param@currpage}{-1}
\setcounter{caption@flags}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{33}
\setcounter{FV@TrueTabGroupLevel}{0}
\setcounter{FV@TrueTabCounter}{0}
\setcounter{FV@HighlightLinesStart}{0}
\setcounter{FV@HighlightLinesStop}{0}
\setcounter{FancyVerbLineBreakLast}{0}
\setcounter{float@type}{16}
\setcounter{minted@FancyVerbLineTemp}{0}
\setcounter{minted@pygmentizecounter}{0}
\setcounter{listing}{0}
\setcounter{parentequation}{0}
\setcounter{NAT@ctr}{0}
\setcounter{section@level}{2}
\setcounter{lstlisting}{0}
}
