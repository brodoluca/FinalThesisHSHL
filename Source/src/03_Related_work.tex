\chapter{Related Work}
Multiple proposed solutions in the academia for sugar beet recognition use Convolutional Neural Networks (CNNs) to recognize both weed and sugar beets in a plantation. The solutions we are going to consider have been studied more thoroughly in \cite{project_work}. \\
\textit{Gao et al. } in \cite{gao_deep_2020} developed a CNN based on the popular tiny YOLOv3 framework (\cite{9074315}), but with modifications for increased performances. They added two more layers for better feature fusion and reduced the number of detection to two, rather than the standard three for YOLO, since the images of sugar beets they collected were generally similar in size.\\
Instead of using only images collected from the field, the authors developed a tool to synthetically generate over 2000 images of sugar beets, which, according to them, allowed them to save time and resources. The model they developed has an accuracy of \textasciitilde83\% with an average inference time of 6.48ms.\\
\textit{Suh et al.} in \cite{suh_transfer_2018} focused their study on the Alexnet architecture (section \ref{sec:Alexnet}) considering three approaches: AlexNet as a fixed feature extractor, modified and fine-tuned AlexNet as a binary classifier, modified and fine-tuned AlexNet as a fixed feature extractor. An overview of the three approaches can be seen in table \ref{tab:alexnet_comparison}. 
\begin{table}[h]
\begin{tabular}[h]{ c c  c c c}
\hline
Approach & Highest accuracy & Training time (s) & Classification time (s) &\# of Images\\
\hline
  1	&	97\%		& 	13.3		&	0.016	&	1100 \\
  2 	& 	98.0\% 	& 	656.4	&	0.012	&	900 \\
  3 	& 	96.7\% 	& 	195.8 	&	0.0130s 	& 	300\\
  3 	& 	97.3\% 	& 	581.4 	&	0.0130s 	& 	800\\

\end{tabular}

\caption[Comparison between the three different approaches using AlexNet]{Comparison between the three different approaches using AlexNet \cite{suh_transfer_2018}}
 \label{tab:alexnet_comparison}
\end{table}

Authors of \cite{suh_transfer_2018} also provide a comparison between six fine-tuned deep networks, namely AlexNet, VGG-19, GoogLeNet, ResNet-50, ResNet-101 and Inception-v3, summarized in table \ref{tab:models_ex_comp}.\\
Both table \ref{tab:alexnet_comparison} and \ref{tab:models_ex_comp} show how differently NNs perform in similar settings when some metrics are changed. For example, in table \ref{tab:models_ex_comp}, we can see that both Resnet50 and Resnet101 achieve different inference time when trained for different numbers of epochs, i.e. with different training time.\\
\begin{table}[h]
\centering
\begin{tabular}{|c| ccc|}
  \hline
 Network &Accuracy& Training time   &Classification time   \\
 \hline
 &\multicolumn{3}{c|}{20 Epoch}\\
 \hline
AlexNet &97.9\%& 9.0min   &0.0038s    \\
VGG-19 &98.4\%& 37.4min   &0.0130s  \\
GoogLeNet &97.0\%& 23.8min  &0.0033s \\
ResNet-50 &96.2\%& 40.3min  &0.0072s\\
ResNet-101 &97.5\%& 106.6min   &0.0118s \\
Inception-v3 &90.8\%& 88.7min &0.0088s \\
\hline
&\multicolumn{3}{c|}{30 Epoch}\\
\hline
AlexNet & 97.7\% &15.6min &0.0040s     \\
VGG-19 &98.7\% &71.4min&0.0124s  \\
GoogLeNet  &97.3\% &36.9min& 0.0035s     \\
ResNet-50 & 97.2\% &69.8min&0.0075s      \\
ResNet-101 & 98.5\% &162.0min&   0.0111s   \\
Inception-v3 & 94.8\% &133.0min   & 0.0086s  \\
\hline
\end{tabular}
\caption[Comparison between the classification performance of different Neural Networks]{Comparison between the classification performance of different Neural Networks \cite{suh_transfer_2018}}
 \label{tab:models_ex_comp}
\end{table}
Regarding the study of Neural Network performances, both academic and industrial organizations have already developed numerous benchmark solutions to evaluate the behaviour of NNs under different workloads and on different devices. For example, authors of \cite{luo2020comparison} and \cite{ignatov2019ai} have proposed benchmarks of NNs specifically for mobile devices. \textit{Hendrycks et al.} in  \cite{hendrycks2019benchmarking} developed a benchmark to assess the robustness of image classifiers under condition of perturbations. \\
\textit{Reddi et al.} in  \cite{reddi2020mlperf} propose a benchmark, namely MLPerf, to evaluate inference of Machine Learning system under various workloads. They divided workloads under high level tasks, such as image classification, and they provide a reference model for it. \\
\textit{Zou et al. } in \cite{8573476}, among other contributions, proposed a benchmark suite to analyse NNs' training time of eight different state-of-the-art models under six different application domains. The metrics they collect during profiling are:
\begin{description}
  \item[\textbf{Throughput}] \hfill\\ Throughput is defined as the amount of input samples processed by the networks. This metric is relevant because, contrary to inference, it is not latency sensitive.   
  \item[\textbf{GPU utilization}] \hfill \\ They define the GPU utilization as the fraction of time in which the GPU is busy, it is calculated as follows:
  \begin{equation}
      GPU\;Utilization = \dfrac{GPU\;Active\;Time\times100}{total\;elapsed\;time}%
  \end{equation}
  
  \item[\textbf{FP32 utilization}] \hfill \\ This metric refers to the percentage of floating operations done during training, since typically training DNNs is performed using single precision floating point operations (FP32). This metric measures how effectively the GPU resources are used. It is calculated as:
  \begin{equation}
      FP32\;Utilization = \dfrac{actual\;flop\;counting\;during\;T\times100}{FLOPS_{peak} \times T}%
  \end{equation}
  Where:
  \begin{itemize}
  \item $FLOPS_{peak}$ is the GPU theoretical peak analysis
  \item $T$ is the period of time in seconds that the GPU is active
  \end{itemize}
  \item[\textbf{CPU utilization }] \hfill\\ This is calculated as the average utilization of each CPU core:
  \begin{equation}
      \dfrac{\sum_{c} total\;time\;of\;active\;core\;c \times 100}{CPU\;core\;count \times total\;elapsed\;time}%
    \end{equation}   
    \item[\textbf{Memory consumption }] \hfill\\ The memory consumed by the NNs during training
\end{description}
Although the aforementioned benchmarks have brought many advancements to the field, they are not meant to be used to study the characteristics of the NNs, but rather to compare and assess NNs performance under different conditions and workloads. Therefore, in future parts of this paper, we will explore techniques to measure NNs performance and determine their behaviour for our own specific goal and under our conditions.  









